{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "## Predict lognormal volatility using different loss functions.  \n",
    "## Apart from MAPELoss function in part 1, we add MAXLoss function and MIXEDLoss function to predict lognormal volatility in order to compare the results between these 3 loss functions. (We only run models and save data here. Comparision is presented in \"evaluation\" notebook)."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"6IBsF7CLN4TyuRNcVu4j6f"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error as MAE, mean_squared_error as MSE, mean_absolute_percentage_error as MAPE"
   ],
   "execution_count":1,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"RHbPuaaIz7Qhk0CCNfEPCn"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "    return device\n",
    "\n",
    "# # convert a df to tensor to be used in pytorch\n",
    "# def df_to_tensor(df):\n",
    "#     device = get_device()\n",
    "#     return torch.from_numpy(df.values).float().to(device)"
   ],
   "execution_count":2,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"UyV3xZySLPiLhGXN0ObgIq"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# trial\n",
    "batch_size = 512\n",
    "n_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# batch_size = 2048\n",
    "# n_epochs = 1000\n",
    "# learning_rate = 0.001"
   ],
   "execution_count":3,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"8pgbffukF9xQc79IqlGWkk"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "data_path = \"\/data\/workspace_files\/\"\n",
    "vols = np.load(data_path + \"12_12_sample_lognormal_vol.npy\")\n",
    "names = [\"S\", \"T\", \"V_atm\", \"Beta\", \"Rho\", \"Volvol\", \"K\"]\n",
    "\n",
    "multiindex = pd.MultiIndex.from_product([range(i) for i in vols.shape],\n",
    "                                        names=names\n",
    "                                       )\n",
    "full_df = pd.DataFrame(vols.reshape((-1,1)), index=multiindex, columns=[\"Lognormal_vol\"]).reset_index()\n",
    "\n",
    "# get features:\n",
    "data_ranges = {'S': np.linspace(0.005+0.0, 0.07+0.03, num=12),\n",
    "               'T': np.linspace(0.5, 20., num=5),\n",
    "               'V_atm': np.linspace(0.001, 0.015, num=3),\n",
    "               'Beta': np.linspace(0.1, 0.7, num=2),\n",
    "               'Rho': np.linspace(-0.4, 0.4, num=3),\n",
    "               'Volvol': np.linspace(0.0001, 0.5, num=5),\n",
    "               'K': np.linspace(0.005+0.0, 0.07+0.03, num=12)\n",
    "              }\n",
    "\n",
    "for key in data_ranges.keys():\n",
    "    full_df[key] = data_ranges[key][full_df[key]]\n",
    "\n",
    "test_df = full_df.sample(frac=0.6, replace=False, random_state=1)\n",
    "print(test_df.shape)\n",
    "train_df = full_df.drop(test_df.index)\n",
    "valid_df = train_df.sample(frac=0.25, replace=False, random_state=1)\n",
    "train_df = train_df.drop(valid_df.index) # train: 30%, valid: 10%, test: 60% (sparse data)\n",
    "\n",
    "train_target = torch.tensor(train_df[['Lognormal_vol']].values.astype(np.float32))\n",
    "train_features = torch.tensor(train_df.drop('Lognormal_vol', axis = 1).values.astype(np.float32)) \n",
    "train_tensor = data_utils.TensorDataset(train_features, train_target) \n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "valid_target = torch.tensor(valid_df[['Lognormal_vol']].values.astype(np.float32))\n",
    "valid_features = torch.tensor(valid_df.drop('Lognormal_vol', axis = 1).values.astype(np.float32)) \n",
    "valid_tensor = data_utils.TensorDataset(valid_features, valid_target) \n",
    "valid_loader = data_utils.DataLoader(dataset = valid_tensor, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "test_target = torch.tensor(test_df[['Lognormal_vol']].values.astype(np.float32))\n",
    "test_features = torch.tensor(test_df.drop('Lognormal_vol', axis = 1).values.astype(np.float32)) \n",
    "test_tensor = data_utils.TensorDataset(test_features, test_target) # revised\n",
    "test_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "loaders = {\"train\": train_loader, \"valid\": valid_loader, \"test\": test_loader}"
   ],
   "execution_count":4,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "(38880, 8)\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"hHqBtabLo5eJVfOrYI2RPT"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, 16)\n",
    "        self.fc2 = nn.Linear(16, 64)\n",
    "        # self.fc3 = nn.Linear(32, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.dropout(out)\n",
    "        # out = F.relu(self.fc3(out))\n",
    "        # out = self.dropout(out)\n",
    "        return self.fc4(out)"
   ],
   "execution_count":5,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"refMqw4KwCdBajrXH0mJDv"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def train(n_epochs, loaders, model, optimizer, criterion, scheduler, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    best_model = model\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 \/ (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + (1 \/ (batch_idx + 1)) * (loss.data - train_loss)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss = valid_loss + (1 \/ (batch_idx + 1)) * (loss.data - valid_loss)\n",
    "        \n",
    "        scheduler.step(loss)\n",
    "\n",
    "        # print training\/validation statistics \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                train_loss,\n",
    "                valid_loss\n",
    "                ))\n",
    "        \n",
    "        ## save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Epoch {}: Validation loss decreased from {:.6f} to {:.6f}.'.format(epoch, valid_loss_min, valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "            best_model = model\n",
    "    if save_path is not None:\n",
    "        torch.save(best_model.state_dict(), save_path)\n",
    "    return best_model\n",
    "    "
   ],
   "execution_count":6,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"LdwzoVtSKa5tU6NS6cfwsr"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "data_path = \"\/data\/workspace_files\/\""
   ],
   "execution_count":7,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"sld1ak34O4Xa7gDnOPDNJi"
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Different Loss Functions Defined Here"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"jC1frtQecEgBVVkuSQkPVj"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def MAPELoss(output, target):\n",
    "    loss = (torch.abs(output - target) \/ torch.abs(target)).mean()\n",
    "    return loss\n",
    "\n",
    "def MAXLoss(output, target):\n",
    "    loss = torch.max(torch.abs(output - target))\n",
    "    return loss\n",
    "\n",
    "def MAPEMAXLoss(output, target, alpha=1.0, beta=1.0):\n",
    "    return alpha * MAPELoss(output, target) + beta * MAXLoss(output, target)"
   ],
   "execution_count":8,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"7m3hHZGQMtbOM97ZGiM2yr"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# model = Net()\n",
    "\n",
    "# def weights_init(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         torch.nn.init.xavier_uniform_(m.weight, gain=torch.nn.init.calculate_gain(\"linear\"))\n",
    "#         m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "# model.apply(weights_init)"
   ],
   "execution_count":9,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"Ao8N3qFnbw7RtUF1O96E6z"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2, last_epoch=-1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, threshold=0.0001, threshold_mode= 'rel', cooldown=0, eps=1e-08)\n",
    "\n",
    "# criterion = MAPEMAXLoss\n",
    "# # criterion = nn.MSELoss()\n",
    "# use_cuda = False"
   ],
   "execution_count":10,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"fDHP9B92impb9zOS0nh5f7"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# # train the model\n",
    "# model = train(n_epochs, loaders, model, optimizer, criterion, scheduler, use_cuda, data_path + 'nn_attempt.pt')\n",
    "\n",
    "# # load the model that got the best validation accuracy\n",
    "# # model.load_state_dict(torch.load(data_path + 'nn_attempt.pt'))"
   ],
   "execution_count":11,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"a7n6cYLvZGZ5X7kvMFg48O"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# pred = model(test_features)\n",
    "\n",
    "# print(MAE(np.squeeze(pred.cpu().detach().numpy()), test_target))\n",
    "# print(MSE(np.squeeze(pred.cpu().detach().numpy()), test_target))\n",
    "# print(MAPE(np.squeeze(pred.cpu().detach().numpy()), test_target))"
   ],
   "execution_count":12,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"0XnJ7Lie9gvQnNom6eu2Rw"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# # Stability Analysis\n",
    "\n",
    "# def run_n(n, model_params):\n",
    "#     # model_params['model']# do it just in case\n",
    "#     runs_mae = []\n",
    "#     runs_mse = []\n",
    "#     runs_mape = []\n",
    "#     for run in range(n):\n",
    "#         model = Net()\n",
    "#         model.apply(model_params['init_fn'])\n",
    "#         model = train(model_params['n_epochs'], model_params['loaders'], \\\n",
    "#         model, torch.optim.Adam(model.parameters(), lr=0.001), model_params['criterion'], \\\n",
    "#         torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, threshold=0.0001, threshold_mode= 'rel', cooldown=0, eps=1e-08),\\\n",
    "#         model_params['use_cuda'], None)\n",
    "\n",
    "#         pred = model(model_params['test_features'])\n",
    "#         runs_mae.append(MAE(np.squeeze(pred.cpu().detach().numpy()), model_params['test_target']))\n",
    "#         runs_mse.append(MSE(np.squeeze(pred.cpu().detach().numpy()), model_params['test_target']))\n",
    "#         runs_mape.append(MAPE(np.squeeze(pred.cpu().detach().numpy()), model_params['test_target']))\n",
    "        \n",
    "#     print(f\"mae mean: {np.mean(runs_mae)} std: {np.std(runs_mae)}\")\n",
    "#     print(f\"mse mean: {np.mean(runs_mse)} std: {np.std(runs_mse)}\")\n",
    "#     print(f\"mape mean: {np.mean(runs_mape)} std: {np.std(runs_mape)}\")"
   ],
   "execution_count":13,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"UgyUs1gCQC8b5wuYOZVBiv"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# # Stability Analysis\n",
    "\n",
    "# model_params = {\n",
    "#     'train_loader': train_loader,\n",
    "#     'test_features': test_features,\n",
    "#     'test_target': test_target,\n",
    "#     'model': Net(),\n",
    "#     # 'optimizer': torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "#     'n_epochs': n_epochs,\n",
    "#     'criterion': MAPEMAXLoss,\n",
    "#     'init_fn': weights_init,\n",
    "#     'loaders': {\"train\": train_loader, \"valid\": valid_loader, \"test\": test_loader},\n",
    "#     'use_cuda': False,\n",
    "#     # 'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, threshold=0.0001, threshold_mode= 'rel', cooldown=0, eps=1e-08)\n",
    "# }\n",
    "\n",
    "# run_n(5, model_params) # trial\n",
    "# # run_n(30, model_params)"
   ],
   "execution_count":14,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"yvnK1zAUVCJCQGNa9LzOmK"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=torch.nn.init.calculate_gain(\"linear\"))\n",
    "        m.bias.data.fill_(0)"
   ],
   "execution_count":15,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"JyvU3TjaGxK0sUQWkhK94m"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "output_df = full_df.copy()\n",
    "output_df[\"is_train\"] = True\n",
    "output_df.loc[test_df.index, \"is_train\"] = False\n",
    "\n",
    "# output_df[\"pred\"] = model(torch.Tensor(full_df[names].values)).detach().numpy()"
   ],
   "execution_count":16,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"Tht1ls4aGNHfSFiLKSgQm4"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "for loss_name, loss_func in zip([\"mapeloss\",\"maxloss\",\"mixedloss\"], [MAPELoss, MAXLoss, MAPEMAXLoss]):\n",
    "    print(f\"\\nTRAIN MODEL USING {loss_name.upper()} LOSS FUNCTION\")\n",
    "    model = Net()\n",
    "    model.apply(weights_init)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, threshold=0.0001, threshold_mode= 'rel', cooldown=0, eps=1e-08)\n",
    "    criterion = loss_func\n",
    "    use_cuda = False\n",
    "    model = train(n_epochs, loaders, model, optimizer, criterion, scheduler, use_cuda, data_path + f'nn_{loss_name}.pt')\n",
    "\n",
    "    output_df[f\"pred_{loss_name}\"] = model(torch.Tensor(full_df[names].values)).detach().numpy()\n",
    "\n",
    "print(\"\\nFINISHED\")"
   ],
   "execution_count":17,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "\n",
      "TRAIN MODEL USING MAPELOSS LOSS FUNCTION\n",
      "Epoch 1: Validation loss decreased from inf to 4.304578.\n",
      "Epoch 2: Validation loss decreased from 4.304578 to 3.386494.\n",
      "Epoch 3: Validation loss decreased from 3.386494 to 1.812892.\n",
      "Epoch 4: Validation loss decreased from 1.812892 to 0.621284.\n",
      "Epoch 5: Validation loss decreased from 0.621284 to 0.345122.\n",
      "Epoch 6: Validation loss decreased from 0.345122 to 0.286533.\n",
      "Epoch 7: Validation loss decreased from 0.286533 to 0.132262.\n",
      "Epoch 8: Validation loss decreased from 0.132262 to 0.095785.\n",
      "Epoch 9: Validation loss decreased from 0.095785 to 0.085397.\n",
      "Epoch: 10 \tTraining Loss: 0.148108 \tValidation Loss: 0.069624\n",
      "Epoch 10: Validation loss decreased from 0.085397 to 0.069624.\n",
      "Epoch 13: Validation loss decreased from 0.069624 to 0.058714.\n",
      "Epoch 15: Validation loss decreased from 0.058714 to 0.046630.\n",
      "Epoch 19: Validation loss decreased from 0.046630 to 0.036272.\n",
      "Epoch: 20 \tTraining Loss: 0.118660 \tValidation Loss: 0.055931\n",
      "Epoch 27: Validation loss decreased from 0.036272 to 0.028106.\n",
      "Epoch: 30 \tTraining Loss: 0.105139 \tValidation Loss: 0.033709\n",
      "Epoch 34: Validation loss decreased from 0.028106 to 0.027503.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 40 \tTraining Loss: 0.096182 \tValidation Loss: 0.035519\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 50 \tTraining Loss: 0.096105 \tValidation Loss: 0.035404\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 60 \tTraining Loss: 0.095425 \tValidation Loss: 0.036585\n",
      "Epoch: 70 \tTraining Loss: 0.095615 \tValidation Loss: 0.036027\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 80 \tTraining Loss: 0.096804 \tValidation Loss: 0.036106\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 90 \tTraining Loss: 0.096289 \tValidation Loss: 0.035974\n",
      "Epoch: 100 \tTraining Loss: 0.095143 \tValidation Loss: 0.035958\n",
      "\n",
      "TRAIN MODEL USING MAXLOSS LOSS FUNCTION\n",
      "Epoch 1: Validation loss decreased from inf to 1.294346.\n",
      "Epoch 4: Validation loss decreased from 1.294346 to 1.217559.\n",
      "Epoch 5: Validation loss decreased from 1.217559 to 0.923875.\n",
      "Epoch 6: Validation loss decreased from 0.923875 to 0.564975.\n",
      "Epoch 7: Validation loss decreased from 0.564975 to 0.474029.\n",
      "Epoch 8: Validation loss decreased from 0.474029 to 0.440717.\n",
      "Epoch 9: Validation loss decreased from 0.440717 to 0.399314.\n",
      "Epoch: 10 \tTraining Loss: 0.512763 \tValidation Loss: 0.343390\n",
      "Epoch 10: Validation loss decreased from 0.399314 to 0.343390.\n",
      "Epoch 11: Validation loss decreased from 0.343390 to 0.311099.\n",
      "Epoch 12: Validation loss decreased from 0.311099 to 0.255771.\n",
      "Epoch 14: Validation loss decreased from 0.255771 to 0.249274.\n",
      "Epoch 15: Validation loss decreased from 0.249274 to 0.229168.\n",
      "Epoch 16: Validation loss decreased from 0.229168 to 0.208356.\n",
      "Epoch: 20 \tTraining Loss: 0.391041 \tValidation Loss: 0.209129\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 30 \tTraining Loss: 0.342758 \tValidation Loss: 0.245715\n",
      "Epoch: 40 \tTraining Loss: 0.326515 \tValidation Loss: 0.231989\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 50 \tTraining Loss: 0.336879 \tValidation Loss: 0.228424\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 60 \tTraining Loss: 0.320562 \tValidation Loss: 0.228688\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 70 \tTraining Loss: 0.320602 \tValidation Loss: 0.228595\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 80 \tTraining Loss: 0.336550 \tValidation Loss: 0.228589\n",
      "Epoch: 90 \tTraining Loss: 0.328288 \tValidation Loss: 0.228587\n",
      "Epoch: 100 \tTraining Loss: 0.322279 \tValidation Loss: 0.228588\n",
      "\n",
      "TRAIN MODEL USING MIXEDLOSS LOSS FUNCTION\n",
      "Epoch 1: Validation loss decreased from inf to 4.964038.\n",
      "Epoch 2: Validation loss decreased from 4.964038 to 2.057895.\n",
      "Epoch 3: Validation loss decreased from 2.057895 to 0.751903.\n",
      "Epoch 4: Validation loss decreased from 0.751903 to 0.405415.\n",
      "Epoch 5: Validation loss decreased from 0.405415 to 0.304936.\n",
      "Epoch 7: Validation loss decreased from 0.304936 to 0.284399.\n",
      "Epoch 8: Validation loss decreased from 0.284399 to 0.269394.\n",
      "Epoch 9: Validation loss decreased from 0.269394 to 0.250717.\n",
      "Epoch: 10 \tTraining Loss: 0.539168 \tValidation Loss: 0.216800\n",
      "Epoch 10: Validation loss decreased from 0.250717 to 0.216800.\n",
      "Epoch 11: Validation loss decreased from 0.216800 to 0.209781.\n",
      "Epoch 12: Validation loss decreased from 0.209781 to 0.186881.\n",
      "Epoch 14: Validation loss decreased from 0.186881 to 0.151901.\n",
      "Epoch: 20 \tTraining Loss: 0.508881 \tValidation Loss: 0.147556\n",
      "Epoch 20: Validation loss decreased from 0.151901 to 0.147556.\n",
      "Epoch 21: Validation loss decreased from 0.147556 to 0.135661.\n",
      "Epoch 22: Validation loss decreased from 0.135661 to 0.128885.\n",
      "Epoch 25: Validation loss decreased from 0.128885 to 0.125130.\n",
      "Epoch 28: Validation loss decreased from 0.125130 to 0.124015.\n",
      "Epoch: 30 \tTraining Loss: 0.502139 \tValidation Loss: 0.164285\n",
      "Epoch 32: Validation loss decreased from 0.124015 to 0.120706.\n",
      "Epoch 33: Validation loss decreased from 0.120706 to 0.116202.\n",
      "Epoch 36: Validation loss decreased from 0.116202 to 0.111601.\n",
      "Epoch: 40 \tTraining Loss: 0.490647 \tValidation Loss: 0.114362\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 50 \tTraining Loss: 0.465125 \tValidation Loss: 0.110605\n",
      "Epoch 50: Validation loss decreased from 0.111601 to 0.110605.\n",
      "Epoch 51: Validation loss decreased from 0.110605 to 0.108229.\n",
      "Epoch 52: Validation loss decreased from 0.108229 to 0.105004.\n",
      "Epoch 53: Validation loss decreased from 0.105004 to 0.104437.\n",
      "Epoch 54: Validation loss decreased from 0.104437 to 0.100656.\n",
      "Epoch 58: Validation loss decreased from 0.100656 to 0.097009.\n",
      "Epoch: 60 \tTraining Loss: 0.461950 \tValidation Loss: 0.110824\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 70 \tTraining Loss: 0.458653 \tValidation Loss: 0.100865\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 80 \tTraining Loss: 0.470453 \tValidation Loss: 0.101253\n",
      "Epoch: 90 \tTraining Loss: 0.461751 \tValidation Loss: 0.100620\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 100 \tTraining Loss: 0.465130 \tValidation Loss: 0.101365\n",
      "\n",
      "FINISHED\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"jO3eWcSOMU9TI25k2qNW9O"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "output_df.head()"
   ],
   "execution_count":18,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>S<\/th>\n",
       "      <th>T<\/th>\n",
       "      <th>V_atm<\/th>\n",
       "      <th>Beta<\/th>\n",
       "      <th>Rho<\/th>\n",
       "      <th>Volvol<\/th>\n",
       "      <th>K<\/th>\n",
       "      <th>Lognormal_vol<\/th>\n",
       "      <th>is_train<\/th>\n",
       "      <th>pred_mapeloss<\/th>\n",
       "      <th>pred_maxloss<\/th>\n",
       "      <th>pred_mixedloss<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>0.005<\/td>\n",
       "      <td>0.5<\/td>\n",
       "      <td>0.001<\/td>\n",
       "      <td>0.1<\/td>\n",
       "      <td>-0.4<\/td>\n",
       "      <td>0.0001<\/td>\n",
       "      <td>0.005000<\/td>\n",
       "      <td>0.198020<\/td>\n",
       "      <td>False<\/td>\n",
       "      <td>0.187524<\/td>\n",
       "      <td>0.378292<\/td>\n",
       "      <td>0.196467<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>0.005<\/td>\n",
       "      <td>0.5<\/td>\n",
       "      <td>0.001<\/td>\n",
       "      <td>0.1<\/td>\n",
       "      <td>-0.4<\/td>\n",
       "      <td>0.0001<\/td>\n",
       "      <td>0.013636<\/td>\n",
       "      <td>0.195679<\/td>\n",
       "      <td>True<\/td>\n",
       "      <td>0.186396<\/td>\n",
       "      <td>0.377358<\/td>\n",
       "      <td>0.194057<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>0.005<\/td>\n",
       "      <td>0.5<\/td>\n",
       "      <td>0.001<\/td>\n",
       "      <td>0.1<\/td>\n",
       "      <td>-0.4<\/td>\n",
       "      <td>0.0001<\/td>\n",
       "      <td>0.022273<\/td>\n",
       "      <td>0.193395<\/td>\n",
       "      <td>False<\/td>\n",
       "      <td>0.185268<\/td>\n",
       "      <td>0.377500<\/td>\n",
       "      <td>0.191647<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>0.005<\/td>\n",
       "      <td>0.5<\/td>\n",
       "      <td>0.001<\/td>\n",
       "      <td>0.1<\/td>\n",
       "      <td>-0.4<\/td>\n",
       "      <td>0.0001<\/td>\n",
       "      <td>0.030909<\/td>\n",
       "      <td>0.191166<\/td>\n",
       "      <td>False<\/td>\n",
       "      <td>0.184139<\/td>\n",
       "      <td>0.377711<\/td>\n",
       "      <td>0.189237<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>0.005<\/td>\n",
       "      <td>0.5<\/td>\n",
       "      <td>0.001<\/td>\n",
       "      <td>0.1<\/td>\n",
       "      <td>-0.4<\/td>\n",
       "      <td>0.0001<\/td>\n",
       "      <td>0.039545<\/td>\n",
       "      <td>0.188990<\/td>\n",
       "      <td>True<\/td>\n",
       "      <td>0.183011<\/td>\n",
       "      <td>0.377922<\/td>\n",
       "      <td>0.187351<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"BRRMJxSfercoT7PbeH6J4J"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "output_df.to_pickle(data_path + \"nn_pred_data.pkl\")"
   ],
   "execution_count":19,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"H9mngekfDmMewEu7xfSHWI"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":0,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"73k7MsFUkEGUnMhSFTI4Yk"
    }
   }
  }
 ],
 "metadata":{
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}