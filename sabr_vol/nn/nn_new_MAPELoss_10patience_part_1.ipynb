{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "## This notebook contains the new neural network that (1) removes three consecutive linear layers in the baseline model. (2) In addition, this model is trained with MAPELoss function instead of MSELoss function. (3) This model uses a learning rate scheduler with patience of 10. (4) Also, we add dropout layers to enhance robostness."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"LXhgcTWytXk4K7ZYh0TUxy"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "execution_count":1,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"RHbPuaaIz7Qhk0CCNfEPCn"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "    return device\n",
    "\n",
    "# # convert a df to tensor to be used in pytorch\n",
    "# def df_to_tensor(df):\n",
    "#     device = get_device()\n",
    "#     return torch.from_numpy(df.values).float().to(device)"
   ],
   "execution_count":2,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"UyV3xZySLPiLhGXN0ObgIq"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "data_path = \"\/data\/workspace_files\/\"\n",
    "vols = np.load(data_path + \"12_12_sample_lognormal_vol.npy\")\n",
    "names = [\"S\", \"T\", \"V_atm\", \"Beta\", \"Rho\", \"Volvol\", \"K\"]\n",
    "\n",
    "multiindex = pd.MultiIndex.from_product([range(i) for i in vols.shape],\n",
    "                                        names=names\n",
    "                                       )\n",
    "full_df = pd.DataFrame(vols.reshape((-1,1)), index=multiindex, columns=[\"Lognormal_vol\"]).reset_index()\n",
    "print(full_df.shape)\n",
    "\n",
    "# get features:\n",
    "data_ranges = {'S': np.linspace(0.005+0.0, 0.07+0.03, num=12),\n",
    "               'T': np.linspace(0.5, 20., num=5),\n",
    "               'V_atm': np.linspace(0.001, 0.015, num=3),\n",
    "               'Beta': np.linspace(0.1, 0.7, num=2),\n",
    "               'Rho': np.linspace(-0.4, 0.4, num=3),\n",
    "               'Volvol': np.linspace(0.0001, 0.5, num=5),\n",
    "               'K': np.linspace(0.005+0.0, 0.07+0.03, num=12)\n",
    "              }\n",
    "\n",
    "for key in data_ranges.keys():\n",
    "    full_df[key] = data_ranges[key][full_df[key]]\n",
    "\n",
    "test_df = full_df.sample(frac=0.6, replace=False, random_state=1)\n",
    "train_df = full_df.drop(test_df.index)\n",
    "valid_df = train_df.sample(frac=0.25, replace=False, random_state=1)\n",
    "train_df = train_df.drop(valid_df.index) # train: 30%, valid: 10%, test: 60% (sparse data)\n",
    "\n",
    "train_target = torch.tensor(train_df[['Lognormal_vol']].values.astype(np.float32))\n",
    "train_features = torch.tensor(train_df.drop('Lognormal_vol', axis = 1).values.astype(np.float32)) \n",
    "train_tensor = data_utils.TensorDataset(train_features, train_target) \n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = 512, shuffle = True) # revised\n",
    "\n",
    "valid_target = torch.tensor(valid_df[['Lognormal_vol']].values.astype(np.float32))\n",
    "valid_features = torch.tensor(valid_df.drop('Lognormal_vol', axis = 1).values.astype(np.float32)) \n",
    "valid_tensor = data_utils.TensorDataset(valid_features, valid_target) \n",
    "valid_loader = data_utils.DataLoader(dataset = valid_tensor, batch_size = 512, shuffle = False) # revised\n",
    "\n",
    "test_target = torch.tensor(test_df[['Lognormal_vol']].values.astype(np.float32))\n",
    "test_features = torch.tensor(test_df.drop('Lognormal_vol', axis = 1).values.astype(np.float32)) \n",
    "test_tensor = data_utils.TensorDataset(test_features, test_target) # revised\n",
    "test_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = 512, shuffle = False) # revised\n",
    "\n",
    "loaders = {\"train\": train_loader, \"valid\": valid_loader, \"test\": test_loader}"
   ],
   "execution_count":3,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "(64800, 8)\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"hHqBtabLo5eJVfOrYI2RPT"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, 16)\n",
    "        self.fc2 = nn.Linear(16, 64)\n",
    "        # self.fc3 = nn.Linear(32, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.dropout(out)\n",
    "        # out = F.relu(self.fc3(out))\n",
    "        # out = self.dropout(out)\n",
    "        return self.fc4(out)"
   ],
   "execution_count":4,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"refMqw4KwCdBajrXH0mJDv"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def train(n_epochs, loaders, model, optimizer, criterion, scheduler, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    best_model = model\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 \/ (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + (1 \/ (batch_idx + 1)) * (loss.data - train_loss)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss = valid_loss + (1 \/ (batch_idx + 1)) * (loss.data - valid_loss)\n",
    "        \n",
    "        scheduler.step(loss)\n",
    "\n",
    "        # print training\/validation statistics \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                train_loss,\n",
    "                valid_loss\n",
    "                ))\n",
    "        \n",
    "        ## save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Epoch {}: Validation loss decreased from {:.6f} to {:.6f}.'.format(epoch, valid_loss_min, valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "            best_model = model\n",
    "    if save_path is not None:\n",
    "        torch.save(best_model.state_dict(), save_path)\n",
    "    return best_model\n",
    "    "
   ],
   "execution_count":5,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"LdwzoVtSKa5tU6NS6cfwsr"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "data_path = \"\/data\/workspace_files\/\""
   ],
   "execution_count":6,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"sld1ak34O4Xa7gDnOPDNJi"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def MAPELoss(output, target):\n",
    "    loss = (torch.abs(output - target) \/ torch.abs(target)).mean()\n",
    "    return loss"
   ],
   "execution_count":7,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"7m3hHZGQMtbOM97ZGiM2yr"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "model = Net()\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=torch.nn.init.calculate_gain(\"linear\"))\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "model.apply(weights_init)"
   ],
   "execution_count":8,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "Net(\n",
       "  (fc1): Linear(in_features=7, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"Ao8N3qFnbw7RtUF1O96E6z"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2, last_epoch=-1)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, threshold=0.0001, threshold_mode= 'rel', cooldown=0, eps=1e-08)\n",
    "n_epochs = 100\n",
    "\n",
    "# from sklearn.metrics import mean_absolute_percentage_error as MAPE\n",
    "# criterion = MAPE\n",
    "criterion = MAPELoss\n",
    "# criterion = nn.MSELoss()\n",
    "use_cuda = False"
   ],
   "execution_count":9,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"fDHP9B92impb9zOS0nh5f7"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# train the model\n",
    "model = train(n_epochs, loaders, model, optimizer, criterion, scheduler, use_cuda, data_path + 'nn_attempt.pt')\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "# model.load_state_dict(torch.load(data_path + 'nn_attempt.pt'))"
   ],
   "execution_count":10,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Epoch 1: Validation loss decreased from inf to 21.785204.\n",
      "Epoch 2: Validation loss decreased from 21.785204 to 2.663019.\n",
      "Epoch 4: Validation loss decreased from 2.663019 to 1.338873.\n",
      "Epoch 5: Validation loss decreased from 1.338873 to 1.072735.\n",
      "Epoch 6: Validation loss decreased from 1.072735 to 0.529367.\n",
      "Epoch 7: Validation loss decreased from 0.529367 to 0.440211.\n",
      "Epoch 8: Validation loss decreased from 0.440211 to 0.314261.\n",
      "Epoch 9: Validation loss decreased from 0.314261 to 0.168549.\n",
      "Epoch: 10 \tTraining Loss: 0.231294 \tValidation Loss: 0.111899\n",
      "Epoch 10: Validation loss decreased from 0.168549 to 0.111899.\n",
      "Epoch 17: Validation loss decreased from 0.111899 to 0.111830.\n",
      "Epoch 18: Validation loss decreased from 0.111830 to 0.099071.\n",
      "Epoch 19: Validation loss decreased from 0.099071 to 0.081912.\n",
      "Epoch: 20 \tTraining Loss: 0.173868 \tValidation Loss: 0.082627\n",
      "Epoch 27: Validation loss decreased from 0.081912 to 0.069057.\n",
      "Epoch: 30 \tTraining Loss: 0.147403 \tValidation Loss: 0.065618\n",
      "Epoch 30: Validation loss decreased from 0.069057 to 0.065618.\n",
      "Epoch 31: Validation loss decreased from 0.065618 to 0.064610.\n",
      "Epoch 34: Validation loss decreased from 0.064610 to 0.053049.\n",
      "Epoch: 40 \tTraining Loss: 0.129907 \tValidation Loss: 0.072548\n",
      "Epoch 43: Validation loss decreased from 0.053049 to 0.051620.\n",
      "Epoch 47: Validation loss decreased from 0.051620 to 0.049051.\n",
      "Epoch: 50 \tTraining Loss: 0.124778 \tValidation Loss: 0.047844\n",
      "Epoch 50: Validation loss decreased from 0.049051 to 0.047844.\n",
      "Epoch 52: Validation loss decreased from 0.047844 to 0.044021.\n",
      "Epoch 59: Validation loss decreased from 0.044021 to 0.039945.\n",
      "Epoch: 60 \tTraining Loss: 0.111255 \tValidation Loss: 0.046319\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 68: Validation loss decreased from 0.039945 to 0.037895.\n",
      "Epoch 69: Validation loss decreased from 0.037895 to 0.037575.\n",
      "Epoch: 70 \tTraining Loss: 0.102957 \tValidation Loss: 0.037880\n",
      "Epoch 71: Validation loss decreased from 0.037575 to 0.036302.\n",
      "Epoch 74: Validation loss decreased from 0.036302 to 0.034461.\n",
      "Epoch: 80 \tTraining Loss: 0.102190 \tValidation Loss: 0.034120\n",
      "Epoch 80: Validation loss decreased from 0.034461 to 0.034120.\n",
      "Epoch 83: Validation loss decreased from 0.034120 to 0.029282.\n",
      "Epoch: 90 \tTraining Loss: 0.099897 \tValidation Loss: 0.032119\n",
      "Epoch 93: Validation loss decreased from 0.029282 to 0.027593.\n",
      "Epoch: 100 \tTraining Loss: 0.100352 \tValidation Loss: 0.038590\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"a7n6cYLvZGZ5X7kvMFg48O"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from sklearn.metrics import mean_absolute_error as MAE, mean_squared_error as MSE, mean_absolute_percentage_error as MAPE\n",
    "\n",
    "pred = model(test_features)\n",
    "\n",
    "print(MAE(np.squeeze(pred.cpu().detach().numpy()), test_target))\n",
    "print(MSE(np.squeeze(pred.cpu().detach().numpy()), test_target))\n",
    "print(MAPE(np.squeeze(pred.cpu().detach().numpy()), test_target))"
   ],
   "execution_count":11,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "0.0056963167\n",
      "0.00026712005\n",
      "0.03936223\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"0XnJ7Lie9gvQnNom6eu2Rw"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def run_n(n, model_params):\n",
    "    # model_params['model']# do it just in case\n",
    "    runs_mae = []\n",
    "    runs_mse = []\n",
    "    runs_mape = []\n",
    "    for run in range(n):\n",
    "        model = Net()\n",
    "        model.apply(model_params['init_fn'])\n",
    "        model = train(model_params['n_epochs'], model_params['loaders'], \\\n",
    "        model, torch.optim.Adam(model.parameters(), lr=0.001), model_params['criterion'], \\\n",
    "        torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, threshold=0.0001, threshold_mode= 'rel', cooldown=0, eps=1e-08),\\\n",
    "        model_params['use_cuda'], None)\n",
    "\n",
    "        pred = model(model_params['test_features'])\n",
    "        runs_mae.append(MAE(np.squeeze(pred.cpu().detach().numpy()), model_params['test_target']))\n",
    "        runs_mse.append(MSE(np.squeeze(pred.cpu().detach().numpy()), model_params['test_target']))\n",
    "        runs_mape.append(MAPE(np.squeeze(pred.cpu().detach().numpy()), model_params['test_target']))\n",
    "        \n",
    "    print(f\"mae mean: {np.mean(runs_mae)} std: {np.std(runs_mae)}\")\n",
    "    print(f\"mse mean: {np.mean(runs_mse)} std: {np.std(runs_mse)}\")\n",
    "    print(f\"mape mean: {np.mean(runs_mape)} std: {np.std(runs_mape)}\")"
   ],
   "execution_count":16,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"UgyUs1gCQC8b5wuYOZVBiv"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "model_params = {\n",
    "    'train_loader': train_loader,\n",
    "    'test_features': test_features,\n",
    "    'test_target': test_target,\n",
    "    'model': Net(),\n",
    "    # 'optimizer': torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "    'n_epochs': 100,\n",
    "    'criterion': MAPELoss,\n",
    "    'init_fn': weights_init,\n",
    "    'loaders': {\"train\": train_loader, \"valid\": valid_loader, \"test\": test_loader},\n",
    "    'use_cuda': False,\n",
    "    # 'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, threshold=0.0001, threshold_mode= 'rel', cooldown=0, eps=1e-08)\n",
    "}\n",
    "\n",
    "run_n(30, model_params)"
   ],
   "execution_count":18,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Epoch 1: Validation loss decreased from inf to 1.462438.\n",
      "Epoch 2: Validation loss decreased from 1.462438 to 1.182286.\n",
      "Epoch 3: Validation loss decreased from 1.182286 to 0.387316.\n",
      "Epoch 4: Validation loss decreased from 0.387316 to 0.193426.\n",
      "Epoch 5: Validation loss decreased from 0.193426 to 0.113042.\n",
      "Epoch 6: Validation loss decreased from 0.113042 to 0.101003.\n",
      "Epoch 9: Validation loss decreased from 0.101003 to 0.095122.\n",
      "Epoch: 10 \tTraining Loss: 0.174444 \tValidation Loss: 0.105714\n",
      "Epoch 11: Validation loss decreased from 0.095122 to 0.085316.\n",
      "Epoch 14: Validation loss decreased from 0.085316 to 0.075383.\n",
      "Epoch 18: Validation loss decreased from 0.075383 to 0.074658.\n",
      "Epoch 19: Validation loss decreased from 0.074658 to 0.066332.\n",
      "Epoch: 20 \tTraining Loss: 0.139585 \tValidation Loss: 0.055579\n",
      "Epoch 20: Validation loss decreased from 0.066332 to 0.055579.\n",
      "Epoch 22: Validation loss decreased from 0.055579 to 0.051207.\n",
      "Epoch 26: Validation loss decreased from 0.051207 to 0.043232.\n",
      "Epoch: 30 \tTraining Loss: 0.126045 \tValidation Loss: 0.056608\n",
      "Epoch 35: Validation loss decreased from 0.043232 to 0.037779.\n",
      "Epoch: 40 \tTraining Loss: 0.120528 \tValidation Loss: 0.051026\n",
      "Epoch 49: Validation loss decreased from 0.037779 to 0.033028.\n",
      "Epoch: 50 \tTraining Loss: 0.118247 \tValidation Loss: 0.034142\n",
      "Epoch: 60 \tTraining Loss: 0.112488 \tValidation Loss: 0.041587\n",
      "Epoch: 70 \tTraining Loss: 0.112887 \tValidation Loss: 0.045401\n",
      "Epoch 74: Validation loss decreased from 0.033028 to 0.032669.\n",
      "Epoch 76: Validation loss decreased from 0.032669 to 0.032394.\n",
      "Epoch: 80 \tTraining Loss: 0.110373 \tValidation Loss: 0.070056\n",
      "Epoch 82: Validation loss decreased from 0.032394 to 0.027165.\n",
      "Epoch: 90 \tTraining Loss: 0.111744 \tValidation Loss: 0.032994\n",
      "Epoch: 100 \tTraining Loss: 0.108329 \tValidation Loss: 0.032054\n",
      "Epoch 1: Validation loss decreased from inf to 11.092480.\n",
      "Epoch 2: Validation loss decreased from 11.092480 to 2.728211.\n",
      "Epoch 3: Validation loss decreased from 2.728211 to 1.602297.\n",
      "Epoch 5: Validation loss decreased from 1.602297 to 0.471630.\n",
      "Epoch 6: Validation loss decreased from 0.471630 to 0.299855.\n",
      "Epoch 7: Validation loss decreased from 0.299855 to 0.201387.\n",
      "Epoch 8: Validation loss decreased from 0.201387 to 0.139127.\n",
      "Epoch 9: Validation loss decreased from 0.139127 to 0.128867.\n",
      "Epoch: 10 \tTraining Loss: 0.183595 \tValidation Loss: 0.117774\n",
      "Epoch 10: Validation loss decreased from 0.128867 to 0.117774.\n",
      "Epoch 12: Validation loss decreased from 0.117774 to 0.084200.\n",
      "Epoch 13: Validation loss decreased from 0.084200 to 0.081765.\n",
      "Epoch 16: Validation loss decreased from 0.081765 to 0.077767.\n",
      "Epoch 17: Validation loss decreased from 0.077767 to 0.076682.\n",
      "Epoch 19: Validation loss decreased from 0.076682 to 0.068425.\n",
      "Epoch: 20 \tTraining Loss: 0.136949 \tValidation Loss: 0.062937\n",
      "Epoch 20: Validation loss decreased from 0.068425 to 0.062937.\n",
      "Epoch 25: Validation loss decreased from 0.062937 to 0.048909.\n",
      "Epoch: 30 \tTraining Loss: 0.125524 \tValidation Loss: 0.066197\n",
      "Epoch: 40 \tTraining Loss: 0.117341 \tValidation Loss: 0.049509\n",
      "Epoch 46: Validation loss decreased from 0.048909 to 0.044419.\n",
      "Epoch 48: Validation loss decreased from 0.044419 to 0.039751.\n",
      "Epoch: 50 \tTraining Loss: 0.109669 \tValidation Loss: 0.044007\n",
      "Epoch 51: Validation loss decreased from 0.039751 to 0.038670.\n",
      "Epoch 55: Validation loss decreased from 0.038670 to 0.037338.\n",
      "Epoch: 60 \tTraining Loss: 0.109670 \tValidation Loss: 0.059233\n",
      "Epoch 66: Validation loss decreased from 0.037338 to 0.032627.\n",
      "Epoch 69: Validation loss decreased from 0.032627 to 0.031183.\n",
      "Epoch: 70 \tTraining Loss: 0.106807 \tValidation Loss: 0.035756\n",
      "Epoch: 80 \tTraining Loss: 0.101565 \tValidation Loss: 0.047793\n",
      "Epoch 81: Validation loss decreased from 0.031183 to 0.029907.\n",
      "Epoch: 90 \tTraining Loss: 0.098552 \tValidation Loss: 0.071098\n",
      "Epoch 95: Validation loss decreased from 0.029907 to 0.024248.\n",
      "Epoch: 100 \tTraining Loss: 0.092093 \tValidation Loss: 0.030185\n",
      "Epoch 1: Validation loss decreased from inf to 3.175242.\n",
      "Epoch 2: Validation loss decreased from 3.175242 to 0.779285.\n",
      "Epoch 3: Validation loss decreased from 0.779285 to 0.377662.\n",
      "Epoch 4: Validation loss decreased from 0.377662 to 0.200840.\n",
      "Epoch 5: Validation loss decreased from 0.200840 to 0.147755.\n",
      "Epoch 6: Validation loss decreased from 0.147755 to 0.110219.\n",
      "Epoch 7: Validation loss decreased from 0.110219 to 0.098658.\n",
      "Epoch 8: Validation loss decreased from 0.098658 to 0.093738.\n",
      "Epoch 9: Validation loss decreased from 0.093738 to 0.076809.\n",
      "Epoch: 10 \tTraining Loss: 0.146741 \tValidation Loss: 0.072177\n",
      "Epoch 10: Validation loss decreased from 0.076809 to 0.072177.\n",
      "Epoch 11: Validation loss decreased from 0.072177 to 0.068939.\n",
      "Epoch 12: Validation loss decreased from 0.068939 to 0.066117.\n",
      "Epoch 15: Validation loss decreased from 0.066117 to 0.053431.\n",
      "Epoch: 20 \tTraining Loss: 0.123169 \tValidation Loss: 0.057565\n",
      "Epoch 22: Validation loss decreased from 0.053431 to 0.047780.\n",
      "Epoch 24: Validation loss decreased from 0.047780 to 0.044294.\n",
      "Epoch 29: Validation loss decreased from 0.044294 to 0.039331.\n",
      "Epoch: 30 \tTraining Loss: 0.112653 \tValidation Loss: 0.043777\n",
      "Epoch 31: Validation loss decreased from 0.039331 to 0.034606.\n",
      "Epoch 38: Validation loss decreased from 0.034606 to 0.030094.\n",
      "Epoch: 40 \tTraining Loss: 0.106494 \tValidation Loss: 0.031725\n",
      "Epoch 45: Validation loss decreased from 0.030094 to 0.028173.\n",
      "Epoch 47: Validation loss decreased from 0.028173 to 0.026052.\n",
      "Epoch: 50 \tTraining Loss: 0.099537 \tValidation Loss: 0.050346\n",
      "Epoch 52: Validation loss decreased from 0.026052 to 0.025467.\n",
      "Epoch: 60 \tTraining Loss: 0.096883 \tValidation Loss: 0.032411\n",
      "Epoch 66: Validation loss decreased from 0.025467 to 0.024811.\n",
      "Epoch: 70 \tTraining Loss: 0.097798 \tValidation Loss: 0.035025\n",
      "Epoch 72: Validation loss decreased from 0.024811 to 0.024529.\n",
      "Epoch: 80 \tTraining Loss: 0.093782 \tValidation Loss: 0.045821\n",
      "Epoch: 90 \tTraining Loss: 0.093414 \tValidation Loss: 0.026030\n",
      "Epoch 93: Validation loss decreased from 0.024529 to 0.022077.\n",
      "Epoch 99: Validation loss decreased from 0.022077 to 0.020291.\n",
      "Epoch: 100 \tTraining Loss: 0.096734 \tValidation Loss: 0.029558\n",
      "Epoch 1: Validation loss decreased from inf to 6.435461.\n",
      "Epoch 2: Validation loss decreased from 6.435461 to 2.947577.\n",
      "Epoch 3: Validation loss decreased from 2.947577 to 2.216461.\n",
      "Epoch 5: Validation loss decreased from 2.216461 to 1.052685.\n",
      "Epoch 6: Validation loss decreased from 1.052685 to 0.827804.\n",
      "Epoch 7: Validation loss decreased from 0.827804 to 0.417918.\n",
      "Epoch 8: Validation loss decreased from 0.417918 to 0.266828.\n",
      "Epoch 9: Validation loss decreased from 0.266828 to 0.165767.\n",
      "Epoch: 10 \tTraining Loss: 0.252050 \tValidation Loss: 0.114418\n",
      "Epoch 10: Validation loss decreased from 0.165767 to 0.114418.\n",
      "Epoch 11: Validation loss decreased from 0.114418 to 0.084710.\n",
      "Epoch 13: Validation loss decreased from 0.084710 to 0.075214.\n",
      "Epoch 14: Validation loss decreased from 0.075214 to 0.072635.\n",
      "Epoch 15: Validation loss decreased from 0.072635 to 0.068306.\n",
      "Epoch 16: Validation loss decreased from 0.068306 to 0.066248.\n",
      "Epoch 18: Validation loss decreased from 0.066248 to 0.054091.\n",
      "Epoch: 20 \tTraining Loss: 0.133184 \tValidation Loss: 0.060656\n",
      "Epoch 23: Validation loss decreased from 0.054091 to 0.046761.\n",
      "Epoch 25: Validation loss decreased from 0.046761 to 0.043001.\n",
      "Epoch 29: Validation loss decreased from 0.043001 to 0.035081.\n",
      "Epoch: 30 \tTraining Loss: 0.115358 \tValidation Loss: 0.044045\n",
      "Epoch 39: Validation loss decreased from 0.035081 to 0.033099.\n",
      "Epoch: 40 \tTraining Loss: 0.110382 \tValidation Loss: 0.046073\n",
      "Epoch 43: Validation loss decreased from 0.033099 to 0.031189.\n",
      "Epoch 46: Validation loss decreased from 0.031189 to 0.029402.\n",
      "Epoch 48: Validation loss decreased from 0.029402 to 0.027183.\n",
      "Epoch: 50 \tTraining Loss: 0.106573 \tValidation Loss: 0.027674\n",
      "Epoch 53: Validation loss decreased from 0.027183 to 0.024983.\n",
      "Epoch: 60 \tTraining Loss: 0.106523 \tValidation Loss: 0.034296\n",
      "Epoch: 70 \tTraining Loss: 0.106186 \tValidation Loss: 0.030623\n",
      "Epoch: 80 \tTraining Loss: 0.104286 \tValidation Loss: 0.047537\n",
      "Epoch: 90 \tTraining Loss: 0.100899 \tValidation Loss: 0.042701\n",
      "Epoch: 100 \tTraining Loss: 0.100439 \tValidation Loss: 0.028124\n",
      "Epoch 1: Validation loss decreased from inf to 1.298883.\n",
      "Epoch 4: Validation loss decreased from 1.298883 to 0.714832.\n",
      "Epoch 5: Validation loss decreased from 0.714832 to 0.536054.\n",
      "Epoch 6: Validation loss decreased from 0.536054 to 0.415532.\n",
      "Epoch 7: Validation loss decreased from 0.415532 to 0.287149.\n",
      "Epoch 8: Validation loss decreased from 0.287149 to 0.137983.\n",
      "Epoch 9: Validation loss decreased from 0.137983 to 0.095134.\n",
      "Epoch: 10 \tTraining Loss: 0.175255 \tValidation Loss: 0.105379\n",
      "Epoch 11: Validation loss decreased from 0.095134 to 0.078659.\n",
      "Epoch 16: Validation loss decreased from 0.078659 to 0.068502.\n",
      "Epoch: 20 \tTraining Loss: 0.148437 \tValidation Loss: 0.075446\n",
      "Epoch 21: Validation loss decreased from 0.068502 to 0.064365.\n",
      "Epoch 27: Validation loss decreased from 0.064365 to 0.062467.\n",
      "Epoch 29: Validation loss decreased from 0.062467 to 0.056230.\n",
      "Epoch: 30 \tTraining Loss: 0.132529 \tValidation Loss: 0.049187\n",
      "Epoch 30: Validation loss decreased from 0.056230 to 0.049187.\n",
      "Epoch 31: Validation loss decreased from 0.049187 to 0.043576.\n",
      "Epoch 35: Validation loss decreased from 0.043576 to 0.039014.\n",
      "Epoch: 40 \tTraining Loss: 0.115056 \tValidation Loss: 0.044625\n",
      "Epoch 44: Validation loss decreased from 0.039014 to 0.035926.\n",
      "Epoch 46: Validation loss decreased from 0.035926 to 0.033982.\n",
      "Epoch 48: Validation loss decreased from 0.033982 to 0.032491.\n",
      "Epoch 49: Validation loss decreased from 0.032491 to 0.031670.\n",
      "Epoch: 50 \tTraining Loss: 0.105809 \tValidation Loss: 0.034395\n",
      "Epoch: 60 \tTraining Loss: 0.100166 \tValidation Loss: 0.046446\n",
      "Epoch: 70 \tTraining Loss: 0.098553 \tValidation Loss: 0.034598\n",
      "Epoch 74: Validation loss decreased from 0.031670 to 0.021812.\n",
      "Epoch: 80 \tTraining Loss: 0.094871 \tValidation Loss: 0.052271\n",
      "Epoch: 90 \tTraining Loss: 0.092880 \tValidation Loss: 0.041658\n",
      "Epoch: 100 \tTraining Loss: 0.091975 \tValidation Loss: 0.035909\n",
      "Epoch 1: Validation loss decreased from inf to 7.773065.\n",
      "Epoch 2: Validation loss decreased from 7.773065 to 2.066578.\n",
      "Epoch 4: Validation loss decreased from 2.066578 to 0.866179.\n",
      "Epoch 5: Validation loss decreased from 0.866179 to 0.198212.\n",
      "Epoch 6: Validation loss decreased from 0.198212 to 0.122290.\n",
      "Epoch 9: Validation loss decreased from 0.122290 to 0.120345.\n",
      "Epoch: 10 \tTraining Loss: 0.190239 \tValidation Loss: 0.107531\n",
      "Epoch 10: Validation loss decreased from 0.120345 to 0.107531.\n",
      "Epoch 12: Validation loss decreased from 0.107531 to 0.100628.\n",
      "Epoch 13: Validation loss decreased from 0.100628 to 0.099594.\n",
      "Epoch 14: Validation loss decreased from 0.099594 to 0.087393.\n",
      "Epoch: 20 \tTraining Loss: 0.160382 \tValidation Loss: 0.102767\n",
      "Epoch 21: Validation loss decreased from 0.087393 to 0.085794.\n",
      "Epoch 23: Validation loss decreased from 0.085794 to 0.071042.\n",
      "Epoch 25: Validation loss decreased from 0.071042 to 0.070473.\n",
      "Epoch 26: Validation loss decreased from 0.070473 to 0.063724.\n",
      "Epoch 28: Validation loss decreased from 0.063724 to 0.058513.\n",
      "Epoch: 30 \tTraining Loss: 0.136345 \tValidation Loss: 0.053266\n",
      "Epoch 30: Validation loss decreased from 0.058513 to 0.053266.\n",
      "Epoch 31: Validation loss decreased from 0.053266 to 0.047597.\n",
      "Epoch: 40 \tTraining Loss: 0.126159 \tValidation Loss: 0.062670\n",
      "Epoch: 50 \tTraining Loss: 0.118776 \tValidation Loss: 0.039972\n",
      "Epoch 50: Validation loss decreased from 0.047597 to 0.039972.\n",
      "Epoch: 60 \tTraining Loss: 0.121306 \tValidation Loss: 0.037060\n",
      "Epoch 60: Validation loss decreased from 0.039972 to 0.037060.\n",
      "Epoch: 70 \tTraining Loss: 0.112027 \tValidation Loss: 0.045365\n",
      "Epoch 76: Validation loss decreased from 0.037060 to 0.036020.\n",
      "Epoch 79: Validation loss decreased from 0.036020 to 0.031793.\n",
      "Epoch: 80 \tTraining Loss: 0.116382 \tValidation Loss: 0.085108\n",
      "Epoch: 90 \tTraining Loss: 0.114270 \tValidation Loss: 0.051606\n",
      "Epoch: 100 \tTraining Loss: 0.104376 \tValidation Loss: 0.044822\n",
      "Epoch 1: Validation loss decreased from inf to 20.790203.\n",
      "Epoch 2: Validation loss decreased from 20.790203 to 2.451239.\n",
      "Epoch 3: Validation loss decreased from 2.451239 to 2.329500.\n",
      "Epoch 4: Validation loss decreased from 2.329500 to 2.119722.\n",
      "Epoch 5: Validation loss decreased from 2.119722 to 1.865133.\n",
      "Epoch 6: Validation loss decreased from 1.865133 to 1.544100.\n",
      "Epoch 7: Validation loss decreased from 1.544100 to 1.284664.\n",
      "Epoch 8: Validation loss decreased from 1.284664 to 0.860079.\n",
      "Epoch 9: Validation loss decreased from 0.860079 to 0.705800.\n",
      "Epoch: 10 \tTraining Loss: 4.183517 \tValidation Loss: 0.450000\n",
      "Epoch 10: Validation loss decreased from 0.705800 to 0.450000.\n",
      "Epoch 11: Validation loss decreased from 0.450000 to 0.210387.\n",
      "Epoch 13: Validation loss decreased from 0.210387 to 0.111620.\n",
      "Epoch 14: Validation loss decreased from 0.111620 to 0.068357.\n",
      "Epoch 15: Validation loss decreased from 0.068357 to 0.056252.\n",
      "Epoch: 20 \tTraining Loss: 0.131828 \tValidation Loss: 0.067779\n",
      "Epoch 23: Validation loss decreased from 0.056252 to 0.053737.\n",
      "Epoch 27: Validation loss decreased from 0.053737 to 0.049180.\n",
      "Epoch 28: Validation loss decreased from 0.049180 to 0.040457.\n",
      "Epoch: 30 \tTraining Loss: 0.120639 \tValidation Loss: 0.046076\n",
      "Epoch 39: Validation loss decreased from 0.040457 to 0.039564.\n",
      "Epoch: 40 \tTraining Loss: 0.116210 \tValidation Loss: 0.049438\n",
      "Epoch: 50 \tTraining Loss: 0.112073 \tValidation Loss: 0.040175\n",
      "Epoch 54: Validation loss decreased from 0.039564 to 0.036543.\n",
      "Epoch 58: Validation loss decreased from 0.036543 to 0.034689.\n",
      "Epoch: 60 \tTraining Loss: 0.108958 \tValidation Loss: 0.036125\n",
      "Epoch: 70 \tTraining Loss: 0.107254 \tValidation Loss: 0.033652\n",
      "Epoch 70: Validation loss decreased from 0.034689 to 0.033652.\n",
      "Epoch 79: Validation loss decreased from 0.033652 to 0.033224.\n",
      "Epoch: 80 \tTraining Loss: 0.103452 \tValidation Loss: 0.042363\n",
      "Epoch 82: Validation loss decreased from 0.033224 to 0.027634.\n",
      "Epoch 83: Validation loss decreased from 0.027634 to 0.025318.\n",
      "Epoch: 90 \tTraining Loss: 0.098975 \tValidation Loss: 0.049255\n",
      "Epoch: 100 \tTraining Loss: 0.094459 \tValidation Loss: 0.031822\n",
      "Epoch 1: Validation loss decreased from inf to 1.890869.\n",
      "Epoch 2: Validation loss decreased from 1.890869 to 0.736025.\n",
      "Epoch 4: Validation loss decreased from 0.736025 to 0.476903.\n",
      "Epoch 5: Validation loss decreased from 0.476903 to 0.179821.\n",
      "Epoch 6: Validation loss decreased from 0.179821 to 0.126643.\n",
      "Epoch 7: Validation loss decreased from 0.126643 to 0.113260.\n",
      "Epoch 9: Validation loss decreased from 0.113260 to 0.095265.\n",
      "Epoch: 10 \tTraining Loss: 0.159116 \tValidation Loss: 0.092484\n",
      "Epoch 10: Validation loss decreased from 0.095265 to 0.092484.\n",
      "Epoch 11: Validation loss decreased from 0.092484 to 0.085739.\n",
      "Epoch 13: Validation loss decreased from 0.085739 to 0.062890.\n",
      "Epoch 14: Validation loss decreased from 0.062890 to 0.054462.\n",
      "Epoch 18: Validation loss decreased from 0.054462 to 0.049918.\n",
      "Epoch 19: Validation loss decreased from 0.049918 to 0.049691.\n",
      "Epoch: 20 \tTraining Loss: 0.135654 \tValidation Loss: 0.076951\n",
      "Epoch 22: Validation loss decreased from 0.049691 to 0.047170.\n",
      "Epoch 23: Validation loss decreased from 0.047170 to 0.044721.\n",
      "Epoch: 30 \tTraining Loss: 0.121534 \tValidation Loss: 0.053110\n",
      "Epoch 31: Validation loss decreased from 0.044721 to 0.036379.\n",
      "Epoch 34: Validation loss decreased from 0.036379 to 0.031596.\n",
      "Epoch: 40 \tTraining Loss: 0.133042 \tValidation Loss: 0.043621\n",
      "Epoch: 50 \tTraining Loss: 0.104352 \tValidation Loss: 0.060288\n",
      "Epoch 52: Validation loss decreased from 0.031596 to 0.030872.\n",
      "Epoch: 60 \tTraining Loss: 0.105996 \tValidation Loss: 0.039912\n",
      "Epoch 68: Validation loss decreased from 0.030872 to 0.029985.\n",
      "Epoch: 70 \tTraining Loss: 0.102938 \tValidation Loss: 0.039565\n",
      "Epoch 76: Validation loss decreased from 0.029985 to 0.028651.\n",
      "Epoch: 80 \tTraining Loss: 0.103522 \tValidation Loss: 0.024559\n",
      "Epoch 80: Validation loss decreased from 0.028651 to 0.024559.\n",
      "Epoch 86: Validation loss decreased from 0.024559 to 0.019956.\n",
      "Epoch 88: Validation loss decreased from 0.019956 to 0.017991.\n",
      "Epoch: 90 \tTraining Loss: 0.096006 \tValidation Loss: 0.032392\n",
      "Epoch: 100 \tTraining Loss: 0.094846 \tValidation Loss: 0.043641\n",
      "Epoch 1: Validation loss decreased from inf to 1.479663.\n",
      "Epoch 2: Validation loss decreased from 1.479663 to 1.162282.\n",
      "Epoch 3: Validation loss decreased from 1.162282 to 1.143291.\n",
      "Epoch 4: Validation loss decreased from 1.143291 to 0.533244.\n",
      "Epoch 7: Validation loss decreased from 0.533244 to 0.259536.\n",
      "Epoch 8: Validation loss decreased from 0.259536 to 0.131667.\n",
      "Epoch 9: Validation loss decreased from 0.131667 to 0.125532.\n",
      "Epoch: 10 \tTraining Loss: 0.181719 \tValidation Loss: 0.096247\n",
      "Epoch 10: Validation loss decreased from 0.125532 to 0.096247.\n",
      "Epoch 11: Validation loss decreased from 0.096247 to 0.056037.\n",
      "Epoch 14: Validation loss decreased from 0.056037 to 0.050648.\n",
      "Epoch 16: Validation loss decreased from 0.050648 to 0.043632.\n",
      "Epoch: 20 \tTraining Loss: 0.121634 \tValidation Loss: 0.042729\n",
      "Epoch 20: Validation loss decreased from 0.043632 to 0.042729.\n",
      "Epoch: 30 \tTraining Loss: 0.116922 \tValidation Loss: 0.050384\n",
      "Epoch 33: Validation loss decreased from 0.042729 to 0.041845.\n",
      "Epoch 36: Validation loss decreased from 0.041845 to 0.038831.\n",
      "Epoch: 40 \tTraining Loss: 0.105294 \tValidation Loss: 0.029772\n",
      "Epoch 40: Validation loss decreased from 0.038831 to 0.029772.\n",
      "Epoch: 50 \tTraining Loss: 0.101053 \tValidation Loss: 0.036765\n",
      "Epoch 57: Validation loss decreased from 0.029772 to 0.024343.\n",
      "Epoch: 60 \tTraining Loss: 0.094846 \tValidation Loss: 0.031953\n",
      "Epoch 63: Validation loss decreased from 0.024343 to 0.024274.\n",
      "Epoch: 70 \tTraining Loss: 0.094059 \tValidation Loss: 0.030258\n",
      "Epoch 78: Validation loss decreased from 0.024274 to 0.017751.\n",
      "Epoch: 80 \tTraining Loss: 0.089766 \tValidation Loss: 0.022538\n",
      "Epoch 88: Validation loss decreased from 0.017751 to 0.017174.\n",
      "Epoch: 90 \tTraining Loss: 0.092031 \tValidation Loss: 0.022543\n",
      "Epoch: 100 \tTraining Loss: 0.086847 \tValidation Loss: 0.022486\n",
      "Epoch 1: Validation loss decreased from inf to 3.366078.\n",
      "Epoch 2: Validation loss decreased from 3.366078 to 1.109308.\n",
      "Epoch 4: Validation loss decreased from 1.109308 to 0.458433.\n",
      "Epoch 5: Validation loss decreased from 0.458433 to 0.376407.\n",
      "Epoch 6: Validation loss decreased from 0.376407 to 0.160961.\n",
      "Epoch 7: Validation loss decreased from 0.160961 to 0.120774.\n",
      "Epoch 8: Validation loss decreased from 0.120774 to 0.096919.\n",
      "Epoch 9: Validation loss decreased from 0.096919 to 0.088904.\n",
      "Epoch: 10 \tTraining Loss: 0.158725 \tValidation Loss: 0.079531\n",
      "Epoch 10: Validation loss decreased from 0.088904 to 0.079531.\n",
      "Epoch 12: Validation loss decreased from 0.079531 to 0.075468.\n",
      "Epoch 14: Validation loss decreased from 0.075468 to 0.070584.\n",
      "Epoch 18: Validation loss decreased from 0.070584 to 0.061414.\n",
      "Epoch: 20 \tTraining Loss: 0.132674 \tValidation Loss: 0.082525\n",
      "Epoch 22: Validation loss decreased from 0.061414 to 0.055158.\n",
      "Epoch 25: Validation loss decreased from 0.055158 to 0.052603.\n",
      "Epoch 26: Validation loss decreased from 0.052603 to 0.052149.\n",
      "Epoch 28: Validation loss decreased from 0.052149 to 0.047459.\n",
      "Epoch 29: Validation loss decreased from 0.047459 to 0.044053.\n",
      "Epoch: 30 \tTraining Loss: 0.124710 \tValidation Loss: 0.068324\n",
      "Epoch: 40 \tTraining Loss: 0.121101 \tValidation Loss: 0.051485\n",
      "Epoch: 50 \tTraining Loss: 0.116882 \tValidation Loss: 0.047963\n",
      "Epoch: 60 \tTraining Loss: 0.108424 \tValidation Loss: 0.054454\n",
      "Epoch: 70 \tTraining Loss: 0.106075 \tValidation Loss: 0.049976\n",
      "Epoch 74: Validation loss decreased from 0.044053 to 0.039971.\n",
      "Epoch: 80 \tTraining Loss: 0.107552 \tValidation Loss: 0.055558\n",
      "Epoch 88: Validation loss decreased from 0.039971 to 0.033490.\n",
      "Epoch: 90 \tTraining Loss: 0.107089 \tValidation Loss: 0.042919\n",
      "Epoch: 100 \tTraining Loss: 0.105270 \tValidation Loss: 0.050703\n",
      "Epoch 1: Validation loss decreased from inf to 3.042877.\n",
      "Epoch 2: Validation loss decreased from 3.042877 to 1.490117.\n",
      "Epoch 3: Validation loss decreased from 1.490117 to 0.771081.\n",
      "Epoch 4: Validation loss decreased from 0.771081 to 0.336263.\n",
      "Epoch 5: Validation loss decreased from 0.336263 to 0.178487.\n",
      "Epoch 6: Validation loss decreased from 0.178487 to 0.145992.\n",
      "Epoch 7: Validation loss decreased from 0.145992 to 0.105304.\n",
      "Epoch 9: Validation loss decreased from 0.105304 to 0.074830.\n",
      "Epoch: 10 \tTraining Loss: 0.149411 \tValidation Loss: 0.075371\n",
      "Epoch 11: Validation loss decreased from 0.074830 to 0.052870.\n",
      "Epoch 14: Validation loss decreased from 0.052870 to 0.047135.\n",
      "Epoch: 20 \tTraining Loss: 0.128875 \tValidation Loss: 0.075533\n",
      "Epoch 24: Validation loss decreased from 0.047135 to 0.044111.\n",
      "Epoch 25: Validation loss decreased from 0.044111 to 0.037736.\n",
      "Epoch: 30 \tTraining Loss: 0.112106 \tValidation Loss: 0.049842\n",
      "Epoch 33: Validation loss decreased from 0.037736 to 0.032998.\n",
      "Epoch 35: Validation loss decreased from 0.032998 to 0.032148.\n",
      "Epoch: 40 \tTraining Loss: 0.103044 \tValidation Loss: 0.045749\n",
      "Epoch 43: Validation loss decreased from 0.032148 to 0.029805.\n",
      "Epoch 48: Validation loss decreased from 0.029805 to 0.029735.\n",
      "Epoch: 50 \tTraining Loss: 0.101054 \tValidation Loss: 0.053334\n",
      "Epoch 53: Validation loss decreased from 0.029735 to 0.029594.\n",
      "Epoch 56: Validation loss decreased from 0.029594 to 0.024547.\n",
      "Epoch: 60 \tTraining Loss: 0.098308 \tValidation Loss: 0.028129\n",
      "Epoch: 70 \tTraining Loss: 0.097540 \tValidation Loss: 0.058590\n",
      "Epoch 77: Validation loss decreased from 0.024547 to 0.019110.\n",
      "Epoch: 80 \tTraining Loss: 0.094557 \tValidation Loss: 0.051017\n",
      "Epoch: 90 \tTraining Loss: 0.096640 \tValidation Loss: 0.034112\n",
      "Epoch: 100 \tTraining Loss: 0.091519 \tValidation Loss: 0.024754\n",
      "Epoch 1: Validation loss decreased from inf to 14.549641.\n",
      "Epoch 2: Validation loss decreased from 14.549641 to 1.919357.\n",
      "Epoch 5: Validation loss decreased from 1.919357 to 1.214860.\n",
      "Epoch 6: Validation loss decreased from 1.214860 to 1.102192.\n",
      "Epoch 7: Validation loss decreased from 1.102192 to 0.649946.\n",
      "Epoch 8: Validation loss decreased from 0.649946 to 0.407758.\n",
      "Epoch 9: Validation loss decreased from 0.407758 to 0.394621.\n",
      "Epoch: 10 \tTraining Loss: 0.377921 \tValidation Loss: 0.297803\n",
      "Epoch 10: Validation loss decreased from 0.394621 to 0.297803.\n",
      "Epoch 11: Validation loss decreased from 0.297803 to 0.169322.\n",
      "Epoch 12: Validation loss decreased from 0.169322 to 0.139738.\n",
      "Epoch 14: Validation loss decreased from 0.139738 to 0.127282.\n",
      "Epoch 17: Validation loss decreased from 0.127282 to 0.119651.\n",
      "Epoch 18: Validation loss decreased from 0.119651 to 0.104688.\n",
      "Epoch: 20 \tTraining Loss: 0.191895 \tValidation Loss: 0.125823\n",
      "Epoch 21: Validation loss decreased from 0.104688 to 0.079847.\n",
      "Epoch 25: Validation loss decreased from 0.079847 to 0.069932.\n",
      "Epoch: 30 \tTraining Loss: 0.146020 \tValidation Loss: 0.070588\n",
      "Epoch 31: Validation loss decreased from 0.069932 to 0.062910.\n",
      "Epoch 35: Validation loss decreased from 0.062910 to 0.060084.\n",
      "Epoch 37: Validation loss decreased from 0.060084 to 0.048384.\n",
      "Epoch: 40 \tTraining Loss: 0.134827 \tValidation Loss: 0.061926\n",
      "Epoch: 50 \tTraining Loss: 0.122768 \tValidation Loss: 0.038752\n",
      "Epoch 50: Validation loss decreased from 0.048384 to 0.038752.\n",
      "Epoch: 60 \tTraining Loss: 0.120249 \tValidation Loss: 0.067533\n",
      "Epoch 61: Validation loss decreased from 0.038752 to 0.037520.\n",
      "Epoch: 70 \tTraining Loss: 0.112734 \tValidation Loss: 0.046917\n",
      "Epoch 71: Validation loss decreased from 0.037520 to 0.033666.\n",
      "Epoch: 80 \tTraining Loss: 0.110156 \tValidation Loss: 0.037361\n",
      "Epoch 87: Validation loss decreased from 0.033666 to 0.026939.\n",
      "Epoch: 90 \tTraining Loss: 0.109848 \tValidation Loss: 0.055375\n",
      "Epoch: 100 \tTraining Loss: 0.106656 \tValidation Loss: 0.056961\n",
      "Epoch 1: Validation loss decreased from inf to 10.909905.\n",
      "Epoch 2: Validation loss decreased from 10.909905 to 3.862820.\n",
      "Epoch 3: Validation loss decreased from 3.862820 to 2.967259.\n",
      "Epoch 4: Validation loss decreased from 2.967259 to 2.309758.\n",
      "Epoch 5: Validation loss decreased from 2.309758 to 0.568928.\n",
      "Epoch 6: Validation loss decreased from 0.568928 to 0.291287.\n",
      "Epoch 7: Validation loss decreased from 0.291287 to 0.192239.\n",
      "Epoch 8: Validation loss decreased from 0.192239 to 0.153324.\n",
      "Epoch 9: Validation loss decreased from 0.153324 to 0.132530.\n",
      "Epoch: 10 \tTraining Loss: 0.208740 \tValidation Loss: 0.123109\n",
      "Epoch 10: Validation loss decreased from 0.132530 to 0.123109.\n",
      "Epoch 11: Validation loss decreased from 0.123109 to 0.122959.\n",
      "Epoch 12: Validation loss decreased from 0.122959 to 0.113579.\n",
      "Epoch 13: Validation loss decreased from 0.113579 to 0.096246.\n",
      "Epoch: 20 \tTraining Loss: 0.176597 \tValidation Loss: 0.100458\n",
      "Epoch 26: Validation loss decreased from 0.096246 to 0.088048.\n",
      "Epoch 28: Validation loss decreased from 0.088048 to 0.082567.\n",
      "Epoch: 30 \tTraining Loss: 0.153926 \tValidation Loss: 0.083031\n",
      "Epoch 31: Validation loss decreased from 0.082567 to 0.064630.\n",
      "Epoch 34: Validation loss decreased from 0.064630 to 0.062237.\n",
      "Epoch: 40 \tTraining Loss: 0.142770 \tValidation Loss: 0.051228\n",
      "Epoch 40: Validation loss decreased from 0.062237 to 0.051228.\n",
      "Epoch: 50 \tTraining Loss: 0.132150 \tValidation Loss: 0.058870\n",
      "Epoch 51: Validation loss decreased from 0.051228 to 0.047371.\n",
      "Epoch: 60 \tTraining Loss: 0.122058 \tValidation Loss: 0.055812\n",
      "Epoch 63: Validation loss decreased from 0.047371 to 0.040318.\n",
      "Epoch: 70 \tTraining Loss: 0.124351 \tValidation Loss: 0.046508\n",
      "Epoch: 80 \tTraining Loss: 0.116284 \tValidation Loss: 0.044958\n",
      "Epoch 84: Validation loss decreased from 0.040318 to 0.034119.\n",
      "Epoch: 90 \tTraining Loss: 0.114681 \tValidation Loss: 0.051924\n",
      "Epoch 91: Validation loss decreased from 0.034119 to 0.030078.\n",
      "Epoch: 100 \tTraining Loss: 0.108457 \tValidation Loss: 0.048923\n",
      "Epoch 1: Validation loss decreased from inf to 4.715403.\n",
      "Epoch 2: Validation loss decreased from 4.715403 to 1.133768.\n",
      "Epoch 5: Validation loss decreased from 1.133768 to 0.795849.\n",
      "Epoch 6: Validation loss decreased from 0.795849 to 0.404671.\n",
      "Epoch 7: Validation loss decreased from 0.404671 to 0.344110.\n",
      "Epoch 8: Validation loss decreased from 0.344110 to 0.184749.\n",
      "Epoch 9: Validation loss decreased from 0.184749 to 0.148956.\n",
      "Epoch: 10 \tTraining Loss: 0.220835 \tValidation Loss: 0.137111\n",
      "Epoch 10: Validation loss decreased from 0.148956 to 0.137111.\n",
      "Epoch 11: Validation loss decreased from 0.137111 to 0.075258.\n",
      "Epoch 17: Validation loss decreased from 0.075258 to 0.066327.\n",
      "Epoch 19: Validation loss decreased from 0.066327 to 0.060730.\n",
      "Epoch: 20 \tTraining Loss: 0.142680 \tValidation Loss: 0.066294\n",
      "Epoch 23: Validation loss decreased from 0.060730 to 0.060229.\n",
      "Epoch 24: Validation loss decreased from 0.060229 to 0.059120.\n",
      "Epoch 27: Validation loss decreased from 0.059120 to 0.055143.\n",
      "Epoch 28: Validation loss decreased from 0.055143 to 0.050773.\n",
      "Epoch: 30 \tTraining Loss: 0.128995 \tValidation Loss: 0.063343\n",
      "Epoch 32: Validation loss decreased from 0.050773 to 0.050200.\n",
      "Epoch 38: Validation loss decreased from 0.050200 to 0.047131.\n",
      "Epoch: 40 \tTraining Loss: 0.120204 \tValidation Loss: 0.050840\n",
      "Epoch 43: Validation loss decreased from 0.047131 to 0.042333.\n",
      "Epoch: 50 \tTraining Loss: 0.115369 \tValidation Loss: 0.043389\n",
      "Epoch 54: Validation loss decreased from 0.042333 to 0.028670.\n",
      "Epoch: 60 \tTraining Loss: 0.111008 \tValidation Loss: 0.043236\n",
      "Epoch: 70 \tTraining Loss: 0.108653 \tValidation Loss: 0.031962\n",
      "Epoch: 80 \tTraining Loss: 0.103901 \tValidation Loss: 0.043733\n",
      "Epoch 87: Validation loss decreased from 0.028670 to 0.028064.\n",
      "Epoch: 90 \tTraining Loss: 0.102698 \tValidation Loss: 0.044670\n",
      "Epoch: 100 \tTraining Loss: 0.100564 \tValidation Loss: 0.024716\n",
      "Epoch 100: Validation loss decreased from 0.028064 to 0.024716.\n",
      "Epoch 1: Validation loss decreased from inf to 3.814482.\n",
      "Epoch 2: Validation loss decreased from 3.814482 to 0.808011.\n",
      "Epoch 4: Validation loss decreased from 0.808011 to 0.750944.\n",
      "Epoch 5: Validation loss decreased from 0.750944 to 0.527246.\n",
      "Epoch 6: Validation loss decreased from 0.527246 to 0.179968.\n",
      "Epoch 7: Validation loss decreased from 0.179968 to 0.113056.\n",
      "Epoch 8: Validation loss decreased from 0.113056 to 0.088085.\n",
      "Epoch 9: Validation loss decreased from 0.088085 to 0.075381.\n",
      "Epoch: 10 \tTraining Loss: 0.147225 \tValidation Loss: 0.086572\n",
      "Epoch 12: Validation loss decreased from 0.075381 to 0.058708.\n",
      "Epoch 15: Validation loss decreased from 0.058708 to 0.052051.\n",
      "Epoch 18: Validation loss decreased from 0.052051 to 0.050511.\n",
      "Epoch: 20 \tTraining Loss: 0.124489 \tValidation Loss: 0.050876\n",
      "Epoch 22: Validation loss decreased from 0.050511 to 0.043866.\n",
      "Epoch 23: Validation loss decreased from 0.043866 to 0.043271.\n",
      "Epoch 25: Validation loss decreased from 0.043271 to 0.042796.\n",
      "Epoch 29: Validation loss decreased from 0.042796 to 0.038495.\n",
      "Epoch: 30 \tTraining Loss: 0.116424 \tValidation Loss: 0.067320\n",
      "Epoch 38: Validation loss decreased from 0.038495 to 0.034682.\n",
      "Epoch: 40 \tTraining Loss: 0.111161 \tValidation Loss: 0.035813\n",
      "Epoch 43: Validation loss decreased from 0.034682 to 0.032779.\n",
      "Epoch: 50 \tTraining Loss: 0.106454 \tValidation Loss: 0.046078\n",
      "Epoch: 60 \tTraining Loss: 0.101643 \tValidation Loss: 0.026438\n",
      "Epoch 60: Validation loss decreased from 0.032779 to 0.026438.\n",
      "Epoch: 70 \tTraining Loss: 0.100567 \tValidation Loss: 0.044930\n",
      "Epoch: 80 \tTraining Loss: 0.096468 \tValidation Loss: 0.052483\n",
      "Epoch: 90 \tTraining Loss: 0.096878 \tValidation Loss: 0.034538\n",
      "Epoch 94: Validation loss decreased from 0.026438 to 0.025450.\n",
      "Epoch: 100 \tTraining Loss: 0.092037 \tValidation Loss: 0.041560\n",
      "Epoch 1: Validation loss decreased from inf to 3.693912.\n",
      "Epoch 3: Validation loss decreased from 3.693912 to 2.660851.\n",
      "Epoch 4: Validation loss decreased from 2.660851 to 0.457908.\n",
      "Epoch 6: Validation loss decreased from 0.457908 to 0.235646.\n",
      "Epoch 7: Validation loss decreased from 0.235646 to 0.159081.\n",
      "Epoch 8: Validation loss decreased from 0.159081 to 0.154330.\n",
      "Epoch 9: Validation loss decreased from 0.154330 to 0.129303.\n",
      "Epoch: 10 \tTraining Loss: 0.193509 \tValidation Loss: 0.148868\n",
      "Epoch 11: Validation loss decreased from 0.129303 to 0.117023.\n",
      "Epoch 13: Validation loss decreased from 0.117023 to 0.093219.\n",
      "Epoch 16: Validation loss decreased from 0.093219 to 0.092706.\n",
      "Epoch 17: Validation loss decreased from 0.092706 to 0.089867.\n",
      "Epoch 18: Validation loss decreased from 0.089867 to 0.070180.\n",
      "Epoch 19: Validation loss decreased from 0.070180 to 0.064840.\n",
      "Epoch: 20 \tTraining Loss: 0.132208 \tValidation Loss: 0.049244\n",
      "Epoch 20: Validation loss decreased from 0.064840 to 0.049244.\n",
      "Epoch 25: Validation loss decreased from 0.049244 to 0.042204.\n",
      "Epoch 27: Validation loss decreased from 0.042204 to 0.037859.\n",
      "Epoch: 30 \tTraining Loss: 0.117420 \tValidation Loss: 0.044923\n",
      "Epoch 35: Validation loss decreased from 0.037859 to 0.035807.\n",
      "Epoch: 40 \tTraining Loss: 0.115715 \tValidation Loss: 0.034109\n",
      "Epoch 40: Validation loss decreased from 0.035807 to 0.034109.\n",
      "Epoch: 50 \tTraining Loss: 0.109560 \tValidation Loss: 0.042189\n",
      "Epoch 58: Validation loss decreased from 0.034109 to 0.033257.\n",
      "Epoch: 60 \tTraining Loss: 0.105810 \tValidation Loss: 0.041933\n",
      "Epoch 62: Validation loss decreased from 0.033257 to 0.030589.\n",
      "Epoch 68: Validation loss decreased from 0.030589 to 0.026986.\n",
      "Epoch: 70 \tTraining Loss: 0.105089 \tValidation Loss: 0.034726\n",
      "Epoch 73: Validation loss decreased from 0.026986 to 0.026975.\n",
      "Epoch: 80 \tTraining Loss: 0.100432 \tValidation Loss: 0.031827\n",
      "Epoch: 90 \tTraining Loss: 0.098540 \tValidation Loss: 0.025294\n",
      "Epoch 90: Validation loss decreased from 0.026975 to 0.025294.\n",
      "Epoch 95: Validation loss decreased from 0.025294 to 0.020033.\n",
      "Epoch: 100 \tTraining Loss: 0.097623 \tValidation Loss: 0.047270\n",
      "Epoch 1: Validation loss decreased from inf to 4.923372.\n",
      "Epoch 2: Validation loss decreased from 4.923372 to 1.946252.\n",
      "Epoch 3: Validation loss decreased from 1.946252 to 0.705675.\n",
      "Epoch 4: Validation loss decreased from 0.705675 to 0.673956.\n",
      "Epoch 5: Validation loss decreased from 0.673956 to 0.196156.\n",
      "Epoch 6: Validation loss decreased from 0.196156 to 0.109185.\n",
      "Epoch 9: Validation loss decreased from 0.109185 to 0.096828.\n",
      "Epoch: 10 \tTraining Loss: 0.165102 \tValidation Loss: 0.089292\n",
      "Epoch 10: Validation loss decreased from 0.096828 to 0.089292.\n",
      "Epoch 12: Validation loss decreased from 0.089292 to 0.069791.\n",
      "Epoch 17: Validation loss decreased from 0.069791 to 0.061079.\n",
      "Epoch 18: Validation loss decreased from 0.061079 to 0.058634.\n",
      "Epoch 19: Validation loss decreased from 0.058634 to 0.049437.\n",
      "Epoch: 20 \tTraining Loss: 0.132748 \tValidation Loss: 0.056400\n",
      "Epoch 26: Validation loss decreased from 0.049437 to 0.042764.\n",
      "Epoch 27: Validation loss decreased from 0.042764 to 0.042527.\n",
      "Epoch: 30 \tTraining Loss: 0.120157 \tValidation Loss: 0.050927\n",
      "Epoch 32: Validation loss decreased from 0.042527 to 0.041008.\n",
      "Epoch 37: Validation loss decreased from 0.041008 to 0.030163.\n",
      "Epoch: 40 \tTraining Loss: 0.110977 \tValidation Loss: 0.046345\n",
      "Epoch 42: Validation loss decreased from 0.030163 to 0.028665.\n",
      "Epoch 43: Validation loss decreased from 0.028665 to 0.027252.\n",
      "Epoch: 50 \tTraining Loss: 0.106366 \tValidation Loss: 0.028020\n",
      "Epoch 51: Validation loss decreased from 0.027252 to 0.026823.\n",
      "Epoch: 60 \tTraining Loss: 0.106141 \tValidation Loss: 0.031384\n",
      "Epoch 65: Validation loss decreased from 0.026823 to 0.023925.\n",
      "Epoch: 70 \tTraining Loss: 0.105149 \tValidation Loss: 0.035302\n",
      "Epoch: 80 \tTraining Loss: 0.101446 \tValidation Loss: 0.051408\n",
      "Epoch: 90 \tTraining Loss: 0.100615 \tValidation Loss: 0.040372\n",
      "Epoch 92: Validation loss decreased from 0.023925 to 0.021839.\n",
      "Epoch: 100 \tTraining Loss: 0.100483 \tValidation Loss: 0.052915\n",
      "Epoch 1: Validation loss decreased from inf to 2.193859.\n",
      "Epoch 2: Validation loss decreased from 2.193859 to 1.154408.\n",
      "Epoch 4: Validation loss decreased from 1.154408 to 0.323646.\n",
      "Epoch 5: Validation loss decreased from 0.323646 to 0.161768.\n",
      "Epoch 6: Validation loss decreased from 0.161768 to 0.129340.\n",
      "Epoch 7: Validation loss decreased from 0.129340 to 0.115762.\n",
      "Epoch 8: Validation loss decreased from 0.115762 to 0.088586.\n",
      "Epoch: 10 \tTraining Loss: 0.163616 \tValidation Loss: 0.127382\n",
      "Epoch 11: Validation loss decreased from 0.088586 to 0.071799.\n",
      "Epoch 13: Validation loss decreased from 0.071799 to 0.056968.\n",
      "Epoch 18: Validation loss decreased from 0.056968 to 0.042089.\n",
      "Epoch: 20 \tTraining Loss: 0.126749 \tValidation Loss: 0.082069\n",
      "Epoch 26: Validation loss decreased from 0.042089 to 0.038285.\n",
      "Epoch: 30 \tTraining Loss: 0.112249 \tValidation Loss: 0.036612\n",
      "Epoch 30: Validation loss decreased from 0.038285 to 0.036612.\n",
      "Epoch 34: Validation loss decreased from 0.036612 to 0.031786.\n",
      "Epoch 36: Validation loss decreased from 0.031786 to 0.029093.\n",
      "Epoch: 40 \tTraining Loss: 0.106319 \tValidation Loss: 0.048484\n",
      "Epoch: 50 \tTraining Loss: 0.105197 \tValidation Loss: 0.033237\n",
      "Epoch: 60 \tTraining Loss: 0.101706 \tValidation Loss: 0.043430\n",
      "Epoch 68: Validation loss decreased from 0.029093 to 0.026829.\n",
      "Epoch: 70 \tTraining Loss: 0.100978 \tValidation Loss: 0.030787\n",
      "Epoch: 80 \tTraining Loss: 0.099839 \tValidation Loss: 0.033115\n",
      "Epoch: 90 \tTraining Loss: 0.102078 \tValidation Loss: 0.035876\n",
      "Epoch 95: Validation loss decreased from 0.026829 to 0.026134.\n",
      "Epoch: 100 \tTraining Loss: 0.096205 \tValidation Loss: 0.033278\n",
      "Epoch 1: Validation loss decreased from inf to 1.603894.\n",
      "Epoch 2: Validation loss decreased from 1.603894 to 1.257225.\n",
      "Epoch 3: Validation loss decreased from 1.257225 to 0.490320.\n",
      "Epoch 4: Validation loss decreased from 0.490320 to 0.280173.\n",
      "Epoch 5: Validation loss decreased from 0.280173 to 0.169357.\n",
      "Epoch 6: Validation loss decreased from 0.169357 to 0.113896.\n",
      "Epoch 8: Validation loss decreased from 0.113896 to 0.084446.\n",
      "Epoch: 10 \tTraining Loss: 0.174382 \tValidation Loss: 0.093269\n",
      "Epoch 14: Validation loss decreased from 0.084446 to 0.070447.\n",
      "Epoch: 20 \tTraining Loss: 0.139111 \tValidation Loss: 0.061019\n",
      "Epoch 20: Validation loss decreased from 0.070447 to 0.061019.\n",
      "Epoch 21: Validation loss decreased from 0.061019 to 0.051224.\n",
      "Epoch 27: Validation loss decreased from 0.051224 to 0.037787.\n",
      "Epoch: 30 \tTraining Loss: 0.123522 \tValidation Loss: 0.034002\n",
      "Epoch 30: Validation loss decreased from 0.037787 to 0.034002.\n",
      "Epoch: 40 \tTraining Loss: 0.125940 \tValidation Loss: 0.072205\n",
      "Epoch: 50 \tTraining Loss: 0.121650 \tValidation Loss: 0.038430\n",
      "Epoch 57: Validation loss decreased from 0.034002 to 0.033493.\n",
      "Epoch: 60 \tTraining Loss: 0.111851 \tValidation Loss: 0.034307\n",
      "Epoch 67: Validation loss decreased from 0.033493 to 0.025743.\n",
      "Epoch: 70 \tTraining Loss: 0.111231 \tValidation Loss: 0.065004\n",
      "Epoch: 80 \tTraining Loss: 0.106168 \tValidation Loss: 0.042450\n",
      "Epoch 85: Validation loss decreased from 0.025743 to 0.024469.\n",
      "Epoch: 90 \tTraining Loss: 0.103899 \tValidation Loss: 0.030106\n",
      "Epoch: 100 \tTraining Loss: 0.103401 \tValidation Loss: 0.045955\n",
      "Epoch 1: Validation loss decreased from inf to 10.250768.\n",
      "Epoch 2: Validation loss decreased from 10.250768 to 2.697912.\n",
      "Epoch 3: Validation loss decreased from 2.697912 to 2.299848.\n",
      "Epoch 4: Validation loss decreased from 2.299848 to 1.009784.\n",
      "Epoch 6: Validation loss decreased from 1.009784 to 0.596043.\n",
      "Epoch 7: Validation loss decreased from 0.596043 to 0.441265.\n",
      "Epoch 8: Validation loss decreased from 0.441265 to 0.139251.\n",
      "Epoch 9: Validation loss decreased from 0.139251 to 0.117218.\n",
      "Epoch: 10 \tTraining Loss: 0.187151 \tValidation Loss: 0.108255\n",
      "Epoch 10: Validation loss decreased from 0.117218 to 0.108255.\n",
      "Epoch 12: Validation loss decreased from 0.108255 to 0.086305.\n",
      "Epoch 17: Validation loss decreased from 0.086305 to 0.078624.\n",
      "Epoch 18: Validation loss decreased from 0.078624 to 0.078138.\n",
      "Epoch: 20 \tTraining Loss: 0.158598 \tValidation Loss: 0.081052\n",
      "Epoch 21: Validation loss decreased from 0.078138 to 0.066343.\n",
      "Epoch 27: Validation loss decreased from 0.066343 to 0.059207.\n",
      "Epoch 29: Validation loss decreased from 0.059207 to 0.057684.\n",
      "Epoch: 30 \tTraining Loss: 0.132410 \tValidation Loss: 0.054019\n",
      "Epoch 30: Validation loss decreased from 0.057684 to 0.054019.\n",
      "Epoch 33: Validation loss decreased from 0.054019 to 0.050551.\n",
      "Epoch: 40 \tTraining Loss: 0.120409 \tValidation Loss: 0.063775\n",
      "Epoch 41: Validation loss decreased from 0.050551 to 0.046274.\n",
      "Epoch 48: Validation loss decreased from 0.046274 to 0.042805.\n",
      "Epoch: 50 \tTraining Loss: 0.112998 \tValidation Loss: 0.054603\n",
      "Epoch 52: Validation loss decreased from 0.042805 to 0.038149.\n",
      "Epoch 55: Validation loss decreased from 0.038149 to 0.035679.\n",
      "Epoch: 60 \tTraining Loss: 0.108763 \tValidation Loss: 0.041385\n",
      "Epoch: 70 \tTraining Loss: 0.106047 \tValidation Loss: 0.050580\n",
      "Epoch 77: Validation loss decreased from 0.035679 to 0.034006.\n",
      "Epoch 78: Validation loss decreased from 0.034006 to 0.029868.\n",
      "Epoch: 80 \tTraining Loss: 0.101433 \tValidation Loss: 0.033423\n",
      "Epoch 88: Validation loss decreased from 0.029868 to 0.028565.\n",
      "Epoch: 90 \tTraining Loss: 0.099405 \tValidation Loss: 0.032330\n",
      "Epoch 97: Validation loss decreased from 0.028565 to 0.028515.\n",
      "Epoch: 100 \tTraining Loss: 0.098240 \tValidation Loss: 0.032056\n",
      "Epoch 1: Validation loss decreased from inf to 5.011275.\n",
      "Epoch 2: Validation loss decreased from 5.011275 to 4.833659.\n",
      "Epoch 3: Validation loss decreased from 4.833659 to 1.342997.\n",
      "Epoch 4: Validation loss decreased from 1.342997 to 0.487268.\n",
      "Epoch 5: Validation loss decreased from 0.487268 to 0.195193.\n",
      "Epoch 6: Validation loss decreased from 0.195193 to 0.135351.\n",
      "Epoch 7: Validation loss decreased from 0.135351 to 0.097526.\n",
      "Epoch 8: Validation loss decreased from 0.097526 to 0.094608.\n",
      "Epoch: 10 \tTraining Loss: 0.164380 \tValidation Loss: 0.092404\n",
      "Epoch 10: Validation loss decreased from 0.094608 to 0.092404.\n",
      "Epoch 11: Validation loss decreased from 0.092404 to 0.075694.\n",
      "Epoch 12: Validation loss decreased from 0.075694 to 0.075518.\n",
      "Epoch 13: Validation loss decreased from 0.075518 to 0.072378.\n",
      "Epoch 15: Validation loss decreased from 0.072378 to 0.063706.\n",
      "Epoch 18: Validation loss decreased from 0.063706 to 0.062250.\n",
      "Epoch 19: Validation loss decreased from 0.062250 to 0.053699.\n",
      "Epoch: 20 \tTraining Loss: 0.136655 \tValidation Loss: 0.075782\n",
      "Epoch 24: Validation loss decreased from 0.053699 to 0.051647.\n",
      "Epoch 25: Validation loss decreased from 0.051647 to 0.045436.\n",
      "Epoch 26: Validation loss decreased from 0.045436 to 0.035121.\n",
      "Epoch: 30 \tTraining Loss: 0.118535 \tValidation Loss: 0.052867\n",
      "Epoch: 40 \tTraining Loss: 0.114171 \tValidation Loss: 0.042908\n",
      "Epoch: 50 \tTraining Loss: 0.108108 \tValidation Loss: 0.038470\n",
      "Epoch 59: Validation loss decreased from 0.035121 to 0.034452.\n",
      "Epoch: 60 \tTraining Loss: 0.103935 \tValidation Loss: 0.044319\n",
      "Epoch 61: Validation loss decreased from 0.034452 to 0.029467.\n",
      "Epoch: 70 \tTraining Loss: 0.102604 \tValidation Loss: 0.043344\n",
      "Epoch 73: Validation loss decreased from 0.029467 to 0.026983.\n",
      "Epoch 75: Validation loss decreased from 0.026983 to 0.022731.\n",
      "Epoch: 80 \tTraining Loss: 0.103044 \tValidation Loss: 0.044960\n",
      "Epoch: 90 \tTraining Loss: 0.099227 \tValidation Loss: 0.034747\n",
      "Epoch 95: Validation loss decreased from 0.022731 to 0.022624.\n",
      "Epoch: 100 \tTraining Loss: 0.097242 \tValidation Loss: 0.046211\n",
      "Epoch 1: Validation loss decreased from inf to 3.413883.\n",
      "Epoch 2: Validation loss decreased from 3.413883 to 2.430872.\n",
      "Epoch 3: Validation loss decreased from 2.430872 to 1.847914.\n",
      "Epoch 4: Validation loss decreased from 1.847914 to 0.880970.\n",
      "Epoch 5: Validation loss decreased from 0.880970 to 0.574035.\n",
      "Epoch 6: Validation loss decreased from 0.574035 to 0.295381.\n",
      "Epoch 7: Validation loss decreased from 0.295381 to 0.130332.\n",
      "Epoch 8: Validation loss decreased from 0.130332 to 0.091742.\n",
      "Epoch 9: Validation loss decreased from 0.091742 to 0.089602.\n",
      "Epoch: 10 \tTraining Loss: 0.172275 \tValidation Loss: 0.084710\n",
      "Epoch 10: Validation loss decreased from 0.089602 to 0.084710.\n",
      "Epoch 12: Validation loss decreased from 0.084710 to 0.084443.\n",
      "Epoch 13: Validation loss decreased from 0.084443 to 0.081400.\n",
      "Epoch 14: Validation loss decreased from 0.081400 to 0.076477.\n",
      "Epoch 16: Validation loss decreased from 0.076477 to 0.065719.\n",
      "Epoch 17: Validation loss decreased from 0.065719 to 0.056031.\n",
      "Epoch: 20 \tTraining Loss: 0.146544 \tValidation Loss: 0.061965\n",
      "Epoch 22: Validation loss decreased from 0.056031 to 0.052885.\n",
      "Epoch 25: Validation loss decreased from 0.052885 to 0.044579.\n",
      "Epoch 29: Validation loss decreased from 0.044579 to 0.042700.\n",
      "Epoch: 30 \tTraining Loss: 0.123916 \tValidation Loss: 0.056875\n",
      "Epoch: 40 \tTraining Loss: 0.118715 \tValidation Loss: 0.074758\n",
      "Epoch 43: Validation loss decreased from 0.042700 to 0.029702.\n",
      "Epoch 48: Validation loss decreased from 0.029702 to 0.028090.\n",
      "Epoch: 50 \tTraining Loss: 0.117212 \tValidation Loss: 0.074546\n",
      "Epoch: 60 \tTraining Loss: 0.116155 \tValidation Loss: 0.040486\n",
      "Epoch: 70 \tTraining Loss: 0.109231 \tValidation Loss: 0.058177\n",
      "Epoch 73: Validation loss decreased from 0.028090 to 0.025004.\n",
      "Epoch: 80 \tTraining Loss: 0.102013 \tValidation Loss: 0.027316\n",
      "Epoch 85: Validation loss decreased from 0.025004 to 0.024965.\n",
      "Epoch 87: Validation loss decreased from 0.024965 to 0.024720.\n",
      "Epoch 88: Validation loss decreased from 0.024720 to 0.022508.\n",
      "Epoch: 90 \tTraining Loss: 0.098670 \tValidation Loss: 0.052997\n",
      "Epoch: 100 \tTraining Loss: 0.094601 \tValidation Loss: 0.040061\n",
      "Epoch 1: Validation loss decreased from inf to 3.170514.\n",
      "Epoch 2: Validation loss decreased from 3.170514 to 2.392225.\n",
      "Epoch 3: Validation loss decreased from 2.392225 to 0.652572.\n",
      "Epoch 4: Validation loss decreased from 0.652572 to 0.368023.\n",
      "Epoch 5: Validation loss decreased from 0.368023 to 0.197000.\n",
      "Epoch 6: Validation loss decreased from 0.197000 to 0.095391.\n",
      "Epoch 7: Validation loss decreased from 0.095391 to 0.070685.\n",
      "Epoch 8: Validation loss decreased from 0.070685 to 0.065808.\n",
      "Epoch: 10 \tTraining Loss: 0.139299 \tValidation Loss: 0.084069\n",
      "Epoch 15: Validation loss decreased from 0.065808 to 0.050846.\n",
      "Epoch 17: Validation loss decreased from 0.050846 to 0.044027.\n",
      "Epoch: 20 \tTraining Loss: 0.113602 \tValidation Loss: 0.038405\n",
      "Epoch 20: Validation loss decreased from 0.044027 to 0.038405.\n",
      "Epoch 22: Validation loss decreased from 0.038405 to 0.030675.\n",
      "Epoch: 30 \tTraining Loss: 0.107783 \tValidation Loss: 0.033035\n",
      "Epoch 33: Validation loss decreased from 0.030675 to 0.024758.\n",
      "Epoch 37: Validation loss decreased from 0.024758 to 0.023414.\n",
      "Epoch: 40 \tTraining Loss: 0.102811 \tValidation Loss: 0.032647\n",
      "Epoch: 50 \tTraining Loss: 0.100835 \tValidation Loss: 0.034278\n",
      "Epoch: 60 \tTraining Loss: 0.097543 \tValidation Loss: 0.033875\n",
      "Epoch: 70 \tTraining Loss: 0.096156 \tValidation Loss: 0.031758\n",
      "Epoch: 80 \tTraining Loss: 0.095019 \tValidation Loss: 0.031881\n",
      "Epoch 86: Validation loss decreased from 0.023414 to 0.023107.\n",
      "Epoch: 90 \tTraining Loss: 0.091681 \tValidation Loss: 0.044838\n",
      "Epoch 98: Validation loss decreased from 0.023107 to 0.019933.\n",
      "Epoch: 100 \tTraining Loss: 0.088143 \tValidation Loss: 0.021095\n",
      "Epoch 1: Validation loss decreased from inf to 5.698349.\n",
      "Epoch 2: Validation loss decreased from 5.698349 to 1.514125.\n",
      "Epoch 4: Validation loss decreased from 1.514125 to 0.948819.\n",
      "Epoch 5: Validation loss decreased from 0.948819 to 0.562392.\n",
      "Epoch 6: Validation loss decreased from 0.562392 to 0.376210.\n",
      "Epoch 7: Validation loss decreased from 0.376210 to 0.177600.\n",
      "Epoch 8: Validation loss decreased from 0.177600 to 0.111444.\n",
      "Epoch 9: Validation loss decreased from 0.111444 to 0.094836.\n",
      "Epoch: 10 \tTraining Loss: 0.171178 \tValidation Loss: 0.125749\n",
      "Epoch 15: Validation loss decreased from 0.094836 to 0.077222.\n",
      "Epoch 18: Validation loss decreased from 0.077222 to 0.067804.\n",
      "Epoch: 20 \tTraining Loss: 0.139755 \tValidation Loss: 0.052485\n",
      "Epoch 20: Validation loss decreased from 0.067804 to 0.052485.\n",
      "Epoch 25: Validation loss decreased from 0.052485 to 0.047162.\n",
      "Epoch 26: Validation loss decreased from 0.047162 to 0.043479.\n",
      "Epoch: 30 \tTraining Loss: 0.118948 \tValidation Loss: 0.044218\n",
      "Epoch: 40 \tTraining Loss: 0.120016 \tValidation Loss: 0.037108\n",
      "Epoch 40: Validation loss decreased from 0.043479 to 0.037108.\n",
      "Epoch 46: Validation loss decreased from 0.037108 to 0.036182.\n",
      "Epoch: 50 \tTraining Loss: 0.104685 \tValidation Loss: 0.032098\n",
      "Epoch 50: Validation loss decreased from 0.036182 to 0.032098.\n",
      "Epoch 58: Validation loss decreased from 0.032098 to 0.029063.\n",
      "Epoch: 60 \tTraining Loss: 0.104187 \tValidation Loss: 0.020772\n",
      "Epoch 60: Validation loss decreased from 0.029063 to 0.020772.\n",
      "Epoch: 70 \tTraining Loss: 0.101461 \tValidation Loss: 0.042177\n",
      "Epoch: 80 \tTraining Loss: 0.100813 \tValidation Loss: 0.042091\n",
      "Epoch: 90 \tTraining Loss: 0.100030 \tValidation Loss: 0.047041\n",
      "Epoch: 100 \tTraining Loss: 0.102338 \tValidation Loss: 0.063585\n",
      "Epoch 1: Validation loss decreased from inf to 6.457059.\n",
      "Epoch 2: Validation loss decreased from 6.457059 to 2.054188.\n",
      "Epoch 3: Validation loss decreased from 2.054188 to 0.951436.\n",
      "Epoch 6: Validation loss decreased from 0.951436 to 0.470519.\n",
      "Epoch 7: Validation loss decreased from 0.470519 to 0.281935.\n",
      "Epoch 8: Validation loss decreased from 0.281935 to 0.204016.\n",
      "Epoch 9: Validation loss decreased from 0.204016 to 0.149271.\n",
      "Epoch: 10 \tTraining Loss: 0.202220 \tValidation Loss: 0.131376\n",
      "Epoch 10: Validation loss decreased from 0.149271 to 0.131376.\n",
      "Epoch 11: Validation loss decreased from 0.131376 to 0.128721.\n",
      "Epoch 12: Validation loss decreased from 0.128721 to 0.115644.\n",
      "Epoch 16: Validation loss decreased from 0.115644 to 0.106251.\n",
      "Epoch: 20 \tTraining Loss: 0.167977 \tValidation Loss: 0.107682\n",
      "Epoch 21: Validation loss decreased from 0.106251 to 0.101385.\n",
      "Epoch 22: Validation loss decreased from 0.101385 to 0.100963.\n",
      "Epoch 27: Validation loss decreased from 0.100963 to 0.098371.\n",
      "Epoch 28: Validation loss decreased from 0.098371 to 0.082160.\n",
      "Epoch: 30 \tTraining Loss: 0.150048 \tValidation Loss: 0.102517\n",
      "Epoch 32: Validation loss decreased from 0.082160 to 0.079375.\n",
      "Epoch 39: Validation loss decreased from 0.079375 to 0.065065.\n",
      "Epoch: 40 \tTraining Loss: 0.143172 \tValidation Loss: 0.085791\n",
      "Epoch 46: Validation loss decreased from 0.065065 to 0.062439.\n",
      "Epoch 48: Validation loss decreased from 0.062439 to 0.060029.\n",
      "Epoch: 50 \tTraining Loss: 0.132565 \tValidation Loss: 0.062215\n",
      "Epoch 51: Validation loss decreased from 0.060029 to 0.055024.\n",
      "Epoch 56: Validation loss decreased from 0.055024 to 0.049731.\n",
      "Epoch: 60 \tTraining Loss: 0.123997 \tValidation Loss: 0.056917\n",
      "Epoch 66: Validation loss decreased from 0.049731 to 0.048848.\n",
      "Epoch 69: Validation loss decreased from 0.048848 to 0.042433.\n",
      "Epoch: 70 \tTraining Loss: 0.123748 \tValidation Loss: 0.058499\n",
      "Epoch: 80 \tTraining Loss: 0.119100 \tValidation Loss: 0.046475\n",
      "Epoch 83: Validation loss decreased from 0.042433 to 0.041063.\n",
      "Epoch 88: Validation loss decreased from 0.041063 to 0.036355.\n",
      "Epoch: 90 \tTraining Loss: 0.116933 \tValidation Loss: 0.049403\n",
      "Epoch 98: Validation loss decreased from 0.036355 to 0.033169.\n",
      "Epoch: 100 \tTraining Loss: 0.112774 \tValidation Loss: 0.051974\n",
      "Epoch 1: Validation loss decreased from inf to 2.681650.\n",
      "Epoch 2: Validation loss decreased from 2.681650 to 1.130334.\n",
      "Epoch 3: Validation loss decreased from 1.130334 to 0.989949.\n",
      "Epoch 4: Validation loss decreased from 0.989949 to 0.793222.\n",
      "Epoch 5: Validation loss decreased from 0.793222 to 0.336604.\n",
      "Epoch 6: Validation loss decreased from 0.336604 to 0.159235.\n",
      "Epoch 7: Validation loss decreased from 0.159235 to 0.135408.\n",
      "Epoch 8: Validation loss decreased from 0.135408 to 0.113541.\n",
      "Epoch 9: Validation loss decreased from 0.113541 to 0.108682.\n",
      "Epoch: 10 \tTraining Loss: 0.181889 \tValidation Loss: 0.123310\n",
      "Epoch 11: Validation loss decreased from 0.108682 to 0.097633.\n",
      "Epoch 13: Validation loss decreased from 0.097633 to 0.093916.\n",
      "Epoch 15: Validation loss decreased from 0.093916 to 0.088063.\n",
      "Epoch 16: Validation loss decreased from 0.088063 to 0.085516.\n",
      "Epoch 17: Validation loss decreased from 0.085516 to 0.080808.\n",
      "Epoch 19: Validation loss decreased from 0.080808 to 0.077042.\n",
      "Epoch: 20 \tTraining Loss: 0.142619 \tValidation Loss: 0.057393\n",
      "Epoch 20: Validation loss decreased from 0.077042 to 0.057393.\n",
      "Epoch 22: Validation loss decreased from 0.057393 to 0.055514.\n",
      "Epoch: 30 \tTraining Loss: 0.118949 \tValidation Loss: 0.050542\n",
      "Epoch 30: Validation loss decreased from 0.055514 to 0.050542.\n",
      "Epoch 33: Validation loss decreased from 0.050542 to 0.042640.\n",
      "Epoch 35: Validation loss decreased from 0.042640 to 0.041060.\n",
      "Epoch 37: Validation loss decreased from 0.041060 to 0.033372.\n",
      "Epoch: 40 \tTraining Loss: 0.114717 \tValidation Loss: 0.045645\n",
      "Epoch: 50 \tTraining Loss: 0.111650 \tValidation Loss: 0.033108\n",
      "Epoch 50: Validation loss decreased from 0.033372 to 0.033108.\n",
      "Epoch 51: Validation loss decreased from 0.033108 to 0.031145.\n",
      "Epoch 54: Validation loss decreased from 0.031145 to 0.029800.\n",
      "Epoch: 60 \tTraining Loss: 0.108461 \tValidation Loss: 0.045010\n",
      "Epoch: 70 \tTraining Loss: 0.108154 \tValidation Loss: 0.038712\n",
      "Epoch 76: Validation loss decreased from 0.029800 to 0.026257.\n",
      "Epoch: 80 \tTraining Loss: 0.106439 \tValidation Loss: 0.038780\n",
      "Epoch: 90 \tTraining Loss: 0.106763 \tValidation Loss: 0.063775\n",
      "Epoch: 100 \tTraining Loss: 0.105588 \tValidation Loss: 0.047275\n",
      "Epoch 1: Validation loss decreased from inf to 3.029410.\n",
      "Epoch 2: Validation loss decreased from 3.029410 to 2.210626.\n",
      "Epoch 3: Validation loss decreased from 2.210626 to 2.046036.\n",
      "Epoch 4: Validation loss decreased from 2.046036 to 0.446575.\n",
      "Epoch 5: Validation loss decreased from 0.446575 to 0.222485.\n",
      "Epoch 6: Validation loss decreased from 0.222485 to 0.190177.\n",
      "Epoch 7: Validation loss decreased from 0.190177 to 0.186975.\n",
      "Epoch 8: Validation loss decreased from 0.186975 to 0.168306.\n",
      "Epoch 9: Validation loss decreased from 0.168306 to 0.152514.\n",
      "Epoch: 10 \tTraining Loss: 0.208826 \tValidation Loss: 0.143197\n",
      "Epoch 10: Validation loss decreased from 0.152514 to 0.143197.\n",
      "Epoch 11: Validation loss decreased from 0.143197 to 0.123437.\n",
      "Epoch 12: Validation loss decreased from 0.123437 to 0.097810.\n",
      "Epoch 15: Validation loss decreased from 0.097810 to 0.095198.\n",
      "Epoch 17: Validation loss decreased from 0.095198 to 0.087825.\n",
      "Epoch: 20 \tTraining Loss: 0.160715 \tValidation Loss: 0.073217\n",
      "Epoch 20: Validation loss decreased from 0.087825 to 0.073217.\n",
      "Epoch 21: Validation loss decreased from 0.073217 to 0.066362.\n",
      "Epoch 26: Validation loss decreased from 0.066362 to 0.058124.\n",
      "Epoch: 30 \tTraining Loss: 0.142551 \tValidation Loss: 0.076107\n",
      "Epoch 35: Validation loss decreased from 0.058124 to 0.055993.\n",
      "Epoch 36: Validation loss decreased from 0.055993 to 0.054483.\n",
      "Epoch: 40 \tTraining Loss: 0.138919 \tValidation Loss: 0.065591\n",
      "Epoch 43: Validation loss decreased from 0.054483 to 0.041888.\n",
      "Epoch: 50 \tTraining Loss: 0.126281 \tValidation Loss: 0.063129\n",
      "Epoch: 60 \tTraining Loss: 0.118545 \tValidation Loss: 0.060101\n",
      "Epoch 69: Validation loss decreased from 0.041888 to 0.038837.\n",
      "Epoch: 70 \tTraining Loss: 0.118487 \tValidation Loss: 0.049512\n",
      "Epoch 74: Validation loss decreased from 0.038837 to 0.036723.\n",
      "Epoch 77: Validation loss decreased from 0.036723 to 0.031273.\n",
      "Epoch: 80 \tTraining Loss: 0.109717 \tValidation Loss: 0.044869\n",
      "Epoch: 90 \tTraining Loss: 0.113139 \tValidation Loss: 0.028055\n",
      "Epoch 90: Validation loss decreased from 0.031273 to 0.028055.\n",
      "Epoch: 100 \tTraining Loss: 0.105292 \tValidation Loss: 0.025634\n",
      "Epoch 100: Validation loss decreased from 0.028055 to 0.025634.\n",
      "Epoch 1: Validation loss decreased from inf to 2.244107.\n",
      "Epoch 3: Validation loss decreased from 2.244107 to 1.249171.\n",
      "Epoch 4: Validation loss decreased from 1.249171 to 0.937451.\n",
      "Epoch 5: Validation loss decreased from 0.937451 to 0.557767.\n",
      "Epoch 6: Validation loss decreased from 0.557767 to 0.253746.\n",
      "Epoch 7: Validation loss decreased from 0.253746 to 0.203832.\n",
      "Epoch 8: Validation loss decreased from 0.203832 to 0.174703.\n",
      "Epoch 9: Validation loss decreased from 0.174703 to 0.145989.\n",
      "Epoch: 10 \tTraining Loss: 0.198060 \tValidation Loss: 0.132743\n",
      "Epoch 10: Validation loss decreased from 0.145989 to 0.132743.\n",
      "Epoch 11: Validation loss decreased from 0.132743 to 0.108723.\n",
      "Epoch 12: Validation loss decreased from 0.108723 to 0.099790.\n",
      "Epoch 14: Validation loss decreased from 0.099790 to 0.092608.\n",
      "Epoch 16: Validation loss decreased from 0.092608 to 0.092021.\n",
      "Epoch 17: Validation loss decreased from 0.092021 to 0.086129.\n",
      "Epoch: 20 \tTraining Loss: 0.151194 \tValidation Loss: 0.081727\n",
      "Epoch 20: Validation loss decreased from 0.086129 to 0.081727.\n",
      "Epoch 21: Validation loss decreased from 0.081727 to 0.081223.\n",
      "Epoch 22: Validation loss decreased from 0.081223 to 0.076488.\n",
      "Epoch 23: Validation loss decreased from 0.076488 to 0.068183.\n",
      "Epoch 24: Validation loss decreased from 0.068183 to 0.066536.\n",
      "Epoch 28: Validation loss decreased from 0.066536 to 0.058258.\n",
      "Epoch: 30 \tTraining Loss: 0.127654 \tValidation Loss: 0.064387\n",
      "Epoch 31: Validation loss decreased from 0.058258 to 0.051496.\n",
      "Epoch 33: Validation loss decreased from 0.051496 to 0.050146.\n",
      "Epoch 39: Validation loss decreased from 0.050146 to 0.046450.\n",
      "Epoch: 40 \tTraining Loss: 0.118662 \tValidation Loss: 0.069510\n",
      "Epoch 46: Validation loss decreased from 0.046450 to 0.043235.\n",
      "Epoch 48: Validation loss decreased from 0.043235 to 0.037813.\n",
      "Epoch: 50 \tTraining Loss: 0.114038 \tValidation Loss: 0.046086\n",
      "Epoch 57: Validation loss decreased from 0.037813 to 0.036848.\n",
      "Epoch: 60 \tTraining Loss: 0.110797 \tValidation Loss: 0.043946\n",
      "Epoch 61: Validation loss decreased from 0.036848 to 0.034388.\n",
      "Epoch 69: Validation loss decreased from 0.034388 to 0.032599.\n",
      "Epoch: 70 \tTraining Loss: 0.108523 \tValidation Loss: 0.043203\n",
      "Epoch: 80 \tTraining Loss: 0.105424 \tValidation Loss: 0.036967\n",
      "Epoch 81: Validation loss decreased from 0.032599 to 0.031953.\n",
      "Epoch 89: Validation loss decreased from 0.031953 to 0.028110.\n",
      "Epoch: 90 \tTraining Loss: 0.097314 \tValidation Loss: 0.028546\n",
      "Epoch: 100 \tTraining Loss: 0.094177 \tValidation Loss: 0.039379\n",
      "Epoch 1: Validation loss decreased from inf to 6.958631.\n",
      "Epoch 2: Validation loss decreased from 6.958631 to 1.231854.\n",
      "Epoch 4: Validation loss decreased from 1.231854 to 0.813895.\n",
      "Epoch 5: Validation loss decreased from 0.813895 to 0.594721.\n",
      "Epoch 6: Validation loss decreased from 0.594721 to 0.364889.\n",
      "Epoch 7: Validation loss decreased from 0.364889 to 0.209068.\n",
      "Epoch 8: Validation loss decreased from 0.209068 to 0.147993.\n",
      "Epoch 9: Validation loss decreased from 0.147993 to 0.136592.\n",
      "Epoch: 10 \tTraining Loss: 0.194690 \tValidation Loss: 0.094174\n",
      "Epoch 10: Validation loss decreased from 0.136592 to 0.094174.\n",
      "Epoch 12: Validation loss decreased from 0.094174 to 0.082970.\n",
      "Epoch 14: Validation loss decreased from 0.082970 to 0.082262.\n",
      "Epoch 15: Validation loss decreased from 0.082262 to 0.062742.\n",
      "Epoch 16: Validation loss decreased from 0.062742 to 0.062011.\n",
      "Epoch 18: Validation loss decreased from 0.062011 to 0.060018.\n",
      "Epoch 19: Validation loss decreased from 0.060018 to 0.055964.\n",
      "Epoch: 20 \tTraining Loss: 0.133563 \tValidation Loss: 0.070498\n",
      "Epoch: 30 \tTraining Loss: 0.120210 \tValidation Loss: 0.063921\n",
      "Epoch 35: Validation loss decreased from 0.055964 to 0.049295.\n",
      "Epoch 37: Validation loss decreased from 0.049295 to 0.040997.\n",
      "Epoch: 40 \tTraining Loss: 0.118618 \tValidation Loss: 0.058928\n",
      "Epoch 47: Validation loss decreased from 0.040997 to 0.040753.\n",
      "Epoch: 50 \tTraining Loss: 0.113462 \tValidation Loss: 0.034127\n",
      "Epoch 50: Validation loss decreased from 0.040753 to 0.034127.\n",
      "Epoch 57: Validation loss decreased from 0.034127 to 0.028539.\n",
      "Epoch 59: Validation loss decreased from 0.028539 to 0.026917.\n",
      "Epoch: 60 \tTraining Loss: 0.109541 \tValidation Loss: 0.058775\n",
      "Epoch 67: Validation loss decreased from 0.026917 to 0.023667.\n",
      "Epoch: 70 \tTraining Loss: 0.109634 \tValidation Loss: 0.036084\n",
      "Epoch: 80 \tTraining Loss: 0.102765 \tValidation Loss: 0.036180\n",
      "Epoch: 90 \tTraining Loss: 0.101106 \tValidation Loss: 0.033165\n",
      "Epoch: 100 \tTraining Loss: 0.099680 \tValidation Loss: 0.037400\n",
      "Epoch 1: Validation loss decreased from inf to 1.849855.\n",
      "Epoch 3: Validation loss decreased from 1.849855 to 0.666706.\n",
      "Epoch 4: Validation loss decreased from 0.666706 to 0.221282.\n",
      "Epoch 5: Validation loss decreased from 0.221282 to 0.187787.\n",
      "Epoch 6: Validation loss decreased from 0.187787 to 0.153591.\n",
      "Epoch 7: Validation loss decreased from 0.153591 to 0.139930.\n",
      "Epoch 9: Validation loss decreased from 0.139930 to 0.124509.\n",
      "Epoch: 10 \tTraining Loss: 0.201846 \tValidation Loss: 0.102272\n",
      "Epoch 10: Validation loss decreased from 0.124509 to 0.102272.\n",
      "Epoch 11: Validation loss decreased from 0.102272 to 0.100430.\n",
      "Epoch 13: Validation loss decreased from 0.100430 to 0.082314.\n",
      "Epoch 15: Validation loss decreased from 0.082314 to 0.072374.\n",
      "Epoch 19: Validation loss decreased from 0.072374 to 0.065934.\n",
      "Epoch: 20 \tTraining Loss: 0.148941 \tValidation Loss: 0.067674\n",
      "Epoch 21: Validation loss decreased from 0.065934 to 0.064805.\n",
      "Epoch 25: Validation loss decreased from 0.064805 to 0.059934.\n",
      "Epoch 28: Validation loss decreased from 0.059934 to 0.057824.\n",
      "Epoch: 30 \tTraining Loss: 0.128854 \tValidation Loss: 0.066562\n",
      "Epoch 31: Validation loss decreased from 0.057824 to 0.044735.\n",
      "Epoch 34: Validation loss decreased from 0.044735 to 0.041923.\n",
      "Epoch 35: Validation loss decreased from 0.041923 to 0.035021.\n",
      "Epoch: 40 \tTraining Loss: 0.128789 \tValidation Loss: 0.049048\n",
      "Epoch 47: Validation loss decreased from 0.035021 to 0.030744.\n",
      "Epoch: 50 \tTraining Loss: 0.113474 \tValidation Loss: 0.058333\n",
      "Epoch: 60 \tTraining Loss: 0.108869 \tValidation Loss: 0.033265\n",
      "Epoch 63: Validation loss decreased from 0.030744 to 0.028619.\n",
      "Epoch: 70 \tTraining Loss: 0.106499 \tValidation Loss: 0.036283\n",
      "Epoch: 80 \tTraining Loss: 0.107720 \tValidation Loss: 0.040484\n",
      "Epoch 82: Validation loss decreased from 0.028619 to 0.027613.\n",
      "Epoch: 90 \tTraining Loss: 0.102477 \tValidation Loss: 0.025365\n",
      "Epoch 90: Validation loss decreased from 0.027613 to 0.025365.\n",
      "Epoch: 100 \tTraining Loss: 0.102633 \tValidation Loss: 0.046082\n",
      "mae mean: 0.005911111831665039 std: 0.0012552834814414382\n",
      "mse mean: 0.0002770460559986532 std: 0.00011091035412391648\n",
      "mape mean: 0.040556926280260086 std: 0.011964475736021996\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"yvnK1zAUVCJCQGNa9LzOmK"
    }
   }
  }
 ],
 "metadata":{
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}