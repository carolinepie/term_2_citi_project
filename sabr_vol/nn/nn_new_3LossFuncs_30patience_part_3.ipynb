{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "## (1) This notebook changes the patience of learning rate scheduler to 30. As patience increases, more epochs are needed to train the neural network. (2) So we increase the number of epochs to 300."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"9nbd0y5JcC91Cz2apZn0N5"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error as MAE, mean_squared_error as MSE, mean_absolute_percentage_error as MAPE"
   ],
   "execution_count":20,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"RHbPuaaIz7Qhk0CCNfEPCn"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU \n",
    "    return device\n",
    "\n",
    "# # convert a df to tensor to be used in pytorch\n",
    "# def df_to_tensor(df):\n",
    "#     device = get_device()\n",
    "#     return torch.from_numpy(df.values).float().to(device)"
   ],
   "execution_count":21,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"UyV3xZySLPiLhGXN0ObgIq"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# trial\n",
    "batch_size = 512\n",
    "n_epochs = 300\n",
    "learning_rate = 0.001\n",
    "\n",
    "# batch_size = 2048\n",
    "# n_epochs = 1000\n",
    "# learning_rate = 0.001"
   ],
   "execution_count":22,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"8pgbffukF9xQc79IqlGWkk"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "data_path = \"\/data\/workspace_files\/\"\n",
    "vols = np.load(data_path + \"12_12_sample_lognormal_vol.npy\")\n",
    "names = [\"S\", \"T\", \"V_atm\", \"Beta\", \"Rho\", \"Volvol\", \"K\"]\n",
    "\n",
    "multiindex = pd.MultiIndex.from_product([range(i) for i in vols.shape],\n",
    "                                        names=names\n",
    "                                       )\n",
    "full_df = pd.DataFrame(vols.reshape((-1,1)), index=multiindex, columns=[\"Lognormal_vol\"]).reset_index()\n",
    "\n",
    "# get features:\n",
    "data_ranges = {'S': np.linspace(0.005+0.0, 0.07+0.03, num=12),\n",
    "               'T': np.linspace(0.5, 20., num=5),\n",
    "               'V_atm': np.linspace(0.001, 0.015, num=3),\n",
    "               'Beta': np.linspace(0.1, 0.7, num=2),\n",
    "               'Rho': np.linspace(-0.4, 0.4, num=3),\n",
    "               'Volvol': np.linspace(0.0001, 0.5, num=5),\n",
    "               'K': np.linspace(0.005+0.0, 0.07+0.03, num=12)\n",
    "              }\n",
    "\n",
    "for key in data_ranges.keys():\n",
    "    full_df[key] = data_ranges[key][full_df[key]]\n",
    "\n",
    "test_df = full_df.sample(frac=0.6, replace=False, random_state=1)\n",
    "print(test_df.shape)\n",
    "train_df = full_df.drop(test_df.index)\n",
    "valid_df = train_df.sample(frac=0.25, replace=False, random_state=1)\n",
    "train_df = train_df.drop(valid_df.index) # train: 30%, valid: 10%, test: 60% (sparse data)\n",
    "\n",
    "train_target = torch.tensor(train_df[['Lognormal_vol']].values.astype(np.float32))\n",
    "train_features = torch.tensor(train_df.drop('Lognormal_vol', axis = 1).values.astype(np.float32)) \n",
    "train_tensor = data_utils.TensorDataset(train_features, train_target) \n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True) \n",
    "\n",
    "valid_target = torch.tensor(valid_df[['Lognormal_vol']].values.astype(np.float32))\n",
    "valid_features = torch.tensor(valid_df.drop('Lognormal_vol', axis = 1).values.astype(np.float32)) \n",
    "valid_tensor = data_utils.TensorDataset(valid_features, valid_target) \n",
    "valid_loader = data_utils.DataLoader(dataset = valid_tensor, batch_size = batch_size, shuffle = False) \n",
    "\n",
    "test_target = torch.tensor(test_df[['Lognormal_vol']].values.astype(np.float32))\n",
    "test_features = torch.tensor(test_df.drop('Lognormal_vol', axis = 1).values.astype(np.float32)) \n",
    "test_tensor = data_utils.TensorDataset(test_features, test_target) # revised\n",
    "test_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = False) \n",
    "\n",
    "loaders = {\"train\": train_loader, \"valid\": valid_loader, \"test\": test_loader}"
   ],
   "execution_count":23,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "(38880, 8)\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"hHqBtabLo5eJVfOrYI2RPT"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(7, 16)\n",
    "        self.fc2 = nn.Linear(16, 64)\n",
    "        # self.fc3 = nn.Linear(32, 64)\n",
    "        self.fc4 = nn.Linear(64, 1)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.dropout(out)\n",
    "        # out = F.relu(self.fc3(out))\n",
    "        # out = self.dropout(out)\n",
    "        return self.fc4(out)"
   ],
   "execution_count":24,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"refMqw4KwCdBajrXH0mJDv"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def train(n_epochs, loaders, model, optimizer, criterion, scheduler, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    best_model = model\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 \/ (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss + (1 \/ (batch_idx + 1)) * (loss.data - train_loss)\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_loss = valid_loss + (1 \/ (batch_idx + 1)) * (loss.data - valid_loss)\n",
    "        \n",
    "        scheduler.step(loss)\n",
    "\n",
    "        # print training\/validation statistics \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "                epoch, \n",
    "                train_loss,\n",
    "                valid_loss\n",
    "                ))\n",
    "        \n",
    "        ## save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Epoch {}: Validation loss decreased from {:.6f} to {:.6f}.'.format(epoch, valid_loss_min, valid_loss))\n",
    "            valid_loss_min = valid_loss\n",
    "            best_model = model\n",
    "    if save_path is not None:\n",
    "        torch.save(best_model.state_dict(), save_path)\n",
    "    return best_model\n",
    "    "
   ],
   "execution_count":25,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"LdwzoVtSKa5tU6NS6cfwsr"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "data_path = \"\/data\/workspace_files\/\""
   ],
   "execution_count":26,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"sld1ak34O4Xa7gDnOPDNJi"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def MAPELoss(output, target):\n",
    "    loss = (torch.abs(output - target) \/ torch.abs(target)).mean()\n",
    "    return loss\n",
    "\n",
    "def MAXLoss(output, target):\n",
    "    loss = torch.max(torch.abs(output - target))\n",
    "    return loss\n",
    "\n",
    "def MAPEMAXLoss(output, target, alpha=1.0, beta=1.0): # revised\n",
    "    return alpha * MAPELoss(output, target) + beta * MAXLoss(output, target)"
   ],
   "execution_count":27,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"7m3hHZGQMtbOM97ZGiM2yr"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# model = Net()\n",
    "\n",
    "# def weights_init(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         torch.nn.init.xavier_uniform_(m.weight, gain=torch.nn.init.calculate_gain(\"linear\"))\n",
    "#         m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "# model.apply(weights_init)"
   ],
   "execution_count":28,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"Ao8N3qFnbw7RtUF1O96E6z"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2, last_epoch=-1)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, threshold=0.0001, threshold_mode= 'rel', cooldown=0, eps=1e-08)\n",
    "\n",
    "# criterion = MAPEMAXLoss\n",
    "# # criterion = nn.MSELoss()\n",
    "# use_cuda = False"
   ],
   "execution_count":29,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"fDHP9B92impb9zOS0nh5f7"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# # train the model\n",
    "# model = train(n_epochs, loaders, model, optimizer, criterion, scheduler, use_cuda, data_path + 'nn_attempt.pt')\n",
    "\n",
    "# # load the model that got the best validation accuracy\n",
    "# # model.load_state_dict(torch.load(data_path + 'nn_attempt.pt'))"
   ],
   "execution_count":30,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"a7n6cYLvZGZ5X7kvMFg48O"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# pred = model(test_features)\n",
    "\n",
    "# print(MAE(np.squeeze(pred.cpu().detach().numpy()), test_target))\n",
    "# print(MSE(np.squeeze(pred.cpu().detach().numpy()), test_target))\n",
    "# print(MAPE(np.squeeze(pred.cpu().detach().numpy()), test_target))"
   ],
   "execution_count":31,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"0XnJ7Lie9gvQnNom6eu2Rw"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# # Stability Analysis\n",
    "\n",
    "# def run_n(n, model_params):\n",
    "#     # model_params['model']# do it just in case\n",
    "#     runs_mae = []\n",
    "#     runs_mse = []\n",
    "#     runs_mape = []\n",
    "#     for run in range(n):\n",
    "#         model = Net()\n",
    "#         model.apply(model_params['init_fn'])\n",
    "#         model = train(model_params['n_epochs'], model_params['loaders'], \\\n",
    "#         model, torch.optim.Adam(model.parameters(), lr=0.001), model_params['criterion'], \\\n",
    "#         torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, threshold=0.0001, threshold_mode= 'rel', cooldown=0, eps=1e-08),\\\n",
    "#         model_params['use_cuda'], None)\n",
    "\n",
    "#         pred = model(model_params['test_features'])\n",
    "#         runs_mae.append(MAE(np.squeeze(pred.cpu().detach().numpy()), model_params['test_target']))\n",
    "#         runs_mse.append(MSE(np.squeeze(pred.cpu().detach().numpy()), model_params['test_target']))\n",
    "#         runs_mape.append(MAPE(np.squeeze(pred.cpu().detach().numpy()), model_params['test_target']))\n",
    "        \n",
    "#     print(f\"mae mean: {np.mean(runs_mae)} std: {np.std(runs_mae)}\")\n",
    "#     print(f\"mse mean: {np.mean(runs_mse)} std: {np.std(runs_mse)}\")\n",
    "#     print(f\"mape mean: {np.mean(runs_mape)} std: {np.std(runs_mape)}\")"
   ],
   "execution_count":32,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"UgyUs1gCQC8b5wuYOZVBiv"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# # Stability Analysis\n",
    "\n",
    "# model_params = {\n",
    "#     'train_loader': train_loader,\n",
    "#     'test_features': test_features,\n",
    "#     'test_target': test_target,\n",
    "#     'model': Net(),\n",
    "#     # 'optimizer': torch.optim.Adam(model.parameters(), lr=0.001),\n",
    "#     'n_epochs': n_epochs,\n",
    "#     'criterion': MAPEMAXLoss,\n",
    "#     'init_fn': weights_init,\n",
    "#     'loaders': {\"train\": train_loader, \"valid\": valid_loader, \"test\": test_loader},\n",
    "#     'use_cuda': False,\n",
    "#     # 'scheduler': torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, threshold=0.0001, threshold_mode= 'rel', cooldown=0, eps=1e-08)\n",
    "# }\n",
    "\n",
    "# run_n(5, model_params) # trial\n",
    "# # run_n(30, model_params)"
   ],
   "execution_count":33,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"yvnK1zAUVCJCQGNa9LzOmK"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=torch.nn.init.calculate_gain(\"linear\"))\n",
    "        m.bias.data.fill_(0)"
   ],
   "execution_count":34,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"JyvU3TjaGxK0sUQWkhK94m"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "output_df = full_df.copy()\n",
    "output_df[\"is_train\"] = True\n",
    "output_df.loc[test_df.index, \"is_train\"] = False\n",
    "\n",
    "# output_df[\"pred\"] = model(torch.Tensor(full_df[names].values)).detach().numpy()"
   ],
   "execution_count":35,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"Tht1ls4aGNHfSFiLKSgQm4"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "for loss_name, loss_func in zip([\"mapeloss\",\"maxloss\",\"mixedloss\"], [MAPELoss, MAXLoss, MAPEMAXLoss]):\n",
    "    print(f\"\\nTRAIN MODEL USING {loss_name.upper()} LOSS FUNCTION\")\n",
    "    model = Net()\n",
    "    model.apply(weights_init)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # revised\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=30, verbose=True, threshold=0.0001, threshold_mode= 'rel', cooldown=0, eps=1e-08)\n",
    "    criterion = loss_func\n",
    "    use_cuda = False\n",
    "    model = train(n_epochs, loaders, model, optimizer, criterion, scheduler, use_cuda, data_path + f'nn_{loss_name}.pt')\n",
    "\n",
    "    output_df[f\"pred_{loss_name}\"] = model(torch.Tensor(full_df[names].values)).detach().numpy()\n",
    "\n",
    "print(\"\\nFINISHED\")"
   ],
   "execution_count":36,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "\n",
      "TRAIN MODEL USING MAPELOSS LOSS FUNCTION\n",
      "Epoch 1: Validation loss decreased from inf to 5.424998.\n",
      "Epoch 2: Validation loss decreased from 5.424998 to 5.348693.\n",
      "Epoch 3: Validation loss decreased from 5.348693 to 4.400710.\n",
      "Epoch 4: Validation loss decreased from 4.400710 to 1.170526.\n",
      "Epoch 5: Validation loss decreased from 1.170526 to 1.001789.\n",
      "Epoch 6: Validation loss decreased from 1.001789 to 0.483139.\n",
      "Epoch 7: Validation loss decreased from 0.483139 to 0.250517.\n",
      "Epoch 8: Validation loss decreased from 0.250517 to 0.193512.\n",
      "Epoch 9: Validation loss decreased from 0.193512 to 0.160922.\n",
      "Epoch: 10 \tTraining Loss: 0.215806 \tValidation Loss: 0.149590\n",
      "Epoch 10: Validation loss decreased from 0.160922 to 0.149590.\n",
      "Epoch 11: Validation loss decreased from 0.149590 to 0.137906.\n",
      "Epoch 12: Validation loss decreased from 0.137906 to 0.128576.\n",
      "Epoch 13: Validation loss decreased from 0.128576 to 0.110631.\n",
      "Epoch 14: Validation loss decreased from 0.110631 to 0.098835.\n",
      "Epoch 18: Validation loss decreased from 0.098835 to 0.095062.\n",
      "Epoch 19: Validation loss decreased from 0.095062 to 0.093838.\n",
      "Epoch: 20 \tTraining Loss: 0.165345 \tValidation Loss: 0.089679\n",
      "Epoch 20: Validation loss decreased from 0.093838 to 0.089679.\n",
      "Epoch 24: Validation loss decreased from 0.089679 to 0.084303.\n",
      "Epoch 26: Validation loss decreased from 0.084303 to 0.078929.\n",
      "Epoch 28: Validation loss decreased from 0.078929 to 0.071383.\n",
      "Epoch 29: Validation loss decreased from 0.071383 to 0.067911.\n",
      "Epoch: 30 \tTraining Loss: 0.137019 \tValidation Loss: 0.075095\n",
      "Epoch 33: Validation loss decreased from 0.067911 to 0.057340.\n",
      "Epoch 36: Validation loss decreased from 0.057340 to 0.053582.\n",
      "Epoch 37: Validation loss decreased from 0.053582 to 0.050513.\n",
      "Epoch 39: Validation loss decreased from 0.050513 to 0.047757.\n",
      "Epoch: 40 \tTraining Loss: 0.127397 \tValidation Loss: 0.049063\n",
      "Epoch 41: Validation loss decreased from 0.047757 to 0.043975.\n",
      "Epoch: 50 \tTraining Loss: 0.124088 \tValidation Loss: 0.050340\n",
      "Epoch 52: Validation loss decreased from 0.043975 to 0.041979.\n",
      "Epoch 55: Validation loss decreased from 0.041979 to 0.040815.\n",
      "Epoch: 60 \tTraining Loss: 0.121645 \tValidation Loss: 0.050884\n",
      "Epoch 61: Validation loss decreased from 0.040815 to 0.039245.\n",
      "Epoch 67: Validation loss decreased from 0.039245 to 0.034211.\n",
      "Epoch: 70 \tTraining Loss: 0.116082 \tValidation Loss: 0.064596\n",
      "Epoch 72: Validation loss decreased from 0.034211 to 0.027773.\n",
      "Epoch 73: Validation loss decreased from 0.027773 to 0.026679.\n",
      "Epoch: 80 \tTraining Loss: 0.108132 \tValidation Loss: 0.034203\n",
      "Epoch: 90 \tTraining Loss: 0.104261 \tValidation Loss: 0.025982\n",
      "Epoch 90: Validation loss decreased from 0.026679 to 0.025982.\n",
      "Epoch: 100 \tTraining Loss: 0.102801 \tValidation Loss: 0.023935\n",
      "Epoch 100: Validation loss decreased from 0.025982 to 0.023935.\n",
      "Epoch: 110 \tTraining Loss: 0.101437 \tValidation Loss: 0.046967\n",
      "Epoch 117: Validation loss decreased from 0.023935 to 0.021460.\n",
      "Epoch: 120 \tTraining Loss: 0.099492 \tValidation Loss: 0.036428\n",
      "Epoch: 130 \tTraining Loss: 0.099175 \tValidation Loss: 0.039914\n",
      "Epoch: 140 \tTraining Loss: 0.099811 \tValidation Loss: 0.029205\n",
      "Epoch 145: Validation loss decreased from 0.021460 to 0.020094.\n",
      "Epoch: 150 \tTraining Loss: 0.100236 \tValidation Loss: 0.042549\n",
      "Epoch: 160 \tTraining Loss: 0.099581 \tValidation Loss: 0.037288\n",
      "Epoch: 170 \tTraining Loss: 0.097165 \tValidation Loss: 0.031870\n",
      "Epoch 173: Validation loss decreased from 0.020094 to 0.017665.\n",
      "Epoch: 180 \tTraining Loss: 0.094313 \tValidation Loss: 0.027269\n",
      "Epoch: 190 \tTraining Loss: 0.095506 \tValidation Loss: 0.036376\n",
      "Epoch: 200 \tTraining Loss: 0.094781 \tValidation Loss: 0.046848\n",
      "Epoch   204: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 210 \tTraining Loss: 0.088233 \tValidation Loss: 0.026763\n",
      "Epoch: 220 \tTraining Loss: 0.089203 \tValidation Loss: 0.030936\n",
      "Epoch: 230 \tTraining Loss: 0.088863 \tValidation Loss: 0.035228\n",
      "Epoch   235: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 240 \tTraining Loss: 0.088217 \tValidation Loss: 0.027843\n",
      "Epoch: 250 \tTraining Loss: 0.088133 \tValidation Loss: 0.027991\n",
      "Epoch: 260 \tTraining Loss: 0.087604 \tValidation Loss: 0.026211\n",
      "Epoch   266: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 270 \tTraining Loss: 0.088127 \tValidation Loss: 0.028467\n",
      "Epoch: 280 \tTraining Loss: 0.087835 \tValidation Loss: 0.028102\n",
      "Epoch: 290 \tTraining Loss: 0.087882 \tValidation Loss: 0.028843\n",
      "Epoch   297: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 300 \tTraining Loss: 0.087665 \tValidation Loss: 0.028383\n",
      "\n",
      "TRAIN MODEL USING MAXLOSS LOSS FUNCTION\n",
      "Epoch 1: Validation loss decreased from inf to 1.243288.\n",
      "Epoch 3: Validation loss decreased from 1.243288 to 0.997753.\n",
      "Epoch 4: Validation loss decreased from 0.997753 to 0.664091.\n",
      "Epoch 5: Validation loss decreased from 0.664091 to 0.569487.\n",
      "Epoch 6: Validation loss decreased from 0.569487 to 0.500190.\n",
      "Epoch 7: Validation loss decreased from 0.500190 to 0.361272.\n",
      "Epoch 8: Validation loss decreased from 0.361272 to 0.255178.\n",
      "Epoch: 10 \tTraining Loss: 0.423626 \tValidation Loss: 0.209873\n",
      "Epoch 10: Validation loss decreased from 0.255178 to 0.209873.\n",
      "Epoch 13: Validation loss decreased from 0.209873 to 0.198623.\n",
      "Epoch 17: Validation loss decreased from 0.198623 to 0.193120.\n",
      "Epoch: 20 \tTraining Loss: 0.347507 \tValidation Loss: 0.181831\n",
      "Epoch 20: Validation loss decreased from 0.193120 to 0.181831.\n",
      "Epoch: 30 \tTraining Loss: 0.325473 \tValidation Loss: 0.218707\n",
      "Epoch: 40 \tTraining Loss: 0.285201 \tValidation Loss: 0.198699\n",
      "Epoch 46: Validation loss decreased from 0.181831 to 0.170496.\n",
      "Epoch 47: Validation loss decreased from 0.170496 to 0.169403.\n",
      "Epoch 49: Validation loss decreased from 0.169403 to 0.149303.\n",
      "Epoch: 50 \tTraining Loss: 0.288312 \tValidation Loss: 0.157530\n",
      "Epoch 51: Validation loss decreased from 0.149303 to 0.139140.\n",
      "Epoch 53: Validation loss decreased from 0.139140 to 0.131663.\n",
      "Epoch 59: Validation loss decreased from 0.131663 to 0.116625.\n",
      "Epoch: 60 \tTraining Loss: 0.267103 \tValidation Loss: 0.126248\n",
      "Epoch 61: Validation loss decreased from 0.116625 to 0.113727.\n",
      "Epoch 66: Validation loss decreased from 0.113727 to 0.106589.\n",
      "Epoch: 70 \tTraining Loss: 0.261418 \tValidation Loss: 0.127018\n",
      "Epoch 73: Validation loss decreased from 0.106589 to 0.092556.\n",
      "Epoch: 80 \tTraining Loss: 0.264522 \tValidation Loss: 0.080999\n",
      "Epoch 80: Validation loss decreased from 0.092556 to 0.080999.\n",
      "Epoch 88: Validation loss decreased from 0.080999 to 0.080488.\n",
      "Epoch: 90 \tTraining Loss: 0.267969 \tValidation Loss: 0.096634\n",
      "Epoch 93: Validation loss decreased from 0.080488 to 0.077252.\n",
      "Epoch 96: Validation loss decreased from 0.077252 to 0.074999.\n",
      "Epoch: 100 \tTraining Loss: 0.253556 \tValidation Loss: 0.086134\n",
      "Epoch 101: Validation loss decreased from 0.074999 to 0.072956.\n",
      "Epoch 103: Validation loss decreased from 0.072956 to 0.072869.\n",
      "Epoch 106: Validation loss decreased from 0.072869 to 0.069610.\n",
      "Epoch: 110 \tTraining Loss: 0.255075 \tValidation Loss: 0.091600\n",
      "Epoch 111: Validation loss decreased from 0.069610 to 0.067143.\n",
      "Epoch: 120 \tTraining Loss: 0.248384 \tValidation Loss: 0.076296\n",
      "Epoch 121: Validation loss decreased from 0.067143 to 0.065273.\n",
      "Epoch: 130 \tTraining Loss: 0.241418 \tValidation Loss: 0.078386\n",
      "Epoch: 140 \tTraining Loss: 0.245723 \tValidation Loss: 0.072289\n",
      "Epoch   142: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 150 \tTraining Loss: 0.231819 \tValidation Loss: 0.076142\n",
      "Epoch: 160 \tTraining Loss: 0.236840 \tValidation Loss: 0.066471\n",
      "Epoch: 170 \tTraining Loss: 0.231771 \tValidation Loss: 0.068308\n",
      "Epoch   173: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 180 \tTraining Loss: 0.236646 \tValidation Loss: 0.071486\n",
      "Epoch: 190 \tTraining Loss: 0.232644 \tValidation Loss: 0.073735\n",
      "Epoch: 200 \tTraining Loss: 0.230603 \tValidation Loss: 0.071549\n",
      "Epoch   204: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 210 \tTraining Loss: 0.236475 \tValidation Loss: 0.071340\n",
      "Epoch: 220 \tTraining Loss: 0.241932 \tValidation Loss: 0.071347\n",
      "Epoch: 230 \tTraining Loss: 0.230054 \tValidation Loss: 0.071268\n",
      "Epoch   235: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 240 \tTraining Loss: 0.229831 \tValidation Loss: 0.071219\n",
      "Epoch: 250 \tTraining Loss: 0.237037 \tValidation Loss: 0.071243\n",
      "Epoch: 260 \tTraining Loss: 0.231779 \tValidation Loss: 0.071271\n",
      "Epoch   266: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 270 \tTraining Loss: 0.234296 \tValidation Loss: 0.071272\n",
      "Epoch: 280 \tTraining Loss: 0.238785 \tValidation Loss: 0.071269\n",
      "Epoch: 290 \tTraining Loss: 0.247678 \tValidation Loss: 0.071270\n",
      "Epoch: 300 \tTraining Loss: 0.227854 \tValidation Loss: 0.071269\n",
      "\n",
      "TRAIN MODEL USING MIXEDLOSS LOSS FUNCTION\n",
      "Epoch 1: Validation loss decreased from inf to 5.541018.\n",
      "Epoch 2: Validation loss decreased from 5.541018 to 3.328285.\n",
      "Epoch 3: Validation loss decreased from 3.328285 to 1.636995.\n",
      "Epoch 4: Validation loss decreased from 1.636995 to 0.494954.\n",
      "Epoch 5: Validation loss decreased from 0.494954 to 0.376592.\n",
      "Epoch 6: Validation loss decreased from 0.376592 to 0.223338.\n",
      "Epoch 7: Validation loss decreased from 0.223338 to 0.206065.\n",
      "Epoch 8: Validation loss decreased from 0.206065 to 0.199781.\n",
      "Epoch 9: Validation loss decreased from 0.199781 to 0.168663.\n",
      "Epoch: 10 \tTraining Loss: 0.599853 \tValidation Loss: 0.214437\n",
      "Epoch 12: Validation loss decreased from 0.168663 to 0.150199.\n",
      "Epoch 14: Validation loss decreased from 0.150199 to 0.121309.\n",
      "Epoch: 20 \tTraining Loss: 0.524437 \tValidation Loss: 0.144124\n",
      "Epoch 21: Validation loss decreased from 0.121309 to 0.120588.\n",
      "Epoch: 30 \tTraining Loss: 0.500360 \tValidation Loss: 0.128869\n",
      "Epoch 33: Validation loss decreased from 0.120588 to 0.107431.\n",
      "Epoch 35: Validation loss decreased from 0.107431 to 0.091834.\n",
      "Epoch: 40 \tTraining Loss: 0.490331 \tValidation Loss: 0.106262\n",
      "Epoch 45: Validation loss decreased from 0.091834 to 0.083468.\n",
      "Epoch: 50 \tTraining Loss: 0.481136 \tValidation Loss: 0.105904\n",
      "Epoch: 60 \tTraining Loss: 0.485563 \tValidation Loss: 0.133922\n",
      "Epoch 64: Validation loss decreased from 0.083468 to 0.078719.\n",
      "Epoch: 70 \tTraining Loss: 0.492548 \tValidation Loss: 0.113034\n",
      "Epoch: 80 \tTraining Loss: 0.474146 \tValidation Loss: 0.105479\n",
      "Epoch 84: Validation loss decreased from 0.078719 to 0.077528.\n",
      "Epoch: 90 \tTraining Loss: 0.483448 \tValidation Loss: 0.105206\n",
      "Epoch 95: Validation loss decreased from 0.077528 to 0.073915.\n",
      "Epoch 99: Validation loss decreased from 0.073915 to 0.063331.\n",
      "Epoch: 100 \tTraining Loss: 0.473577 \tValidation Loss: 0.094140\n",
      "Epoch: 110 \tTraining Loss: 0.478317 \tValidation Loss: 0.108748\n",
      "Epoch: 120 \tTraining Loss: 0.475259 \tValidation Loss: 0.088889\n",
      "Epoch: 130 \tTraining Loss: 0.471686 \tValidation Loss: 0.075219\n",
      "Epoch: 140 \tTraining Loss: 0.474017 \tValidation Loss: 0.070173\n",
      "Epoch 142: Validation loss decreased from 0.063331 to 0.057443.\n",
      "Epoch: 150 \tTraining Loss: 0.455434 \tValidation Loss: 0.106457\n",
      "Epoch: 160 \tTraining Loss: 0.456771 \tValidation Loss: 0.088080\n",
      "Epoch: 170 \tTraining Loss: 0.466665 \tValidation Loss: 0.064956\n",
      "Epoch   173: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 180 \tTraining Loss: 0.447509 \tValidation Loss: 0.080592\n",
      "Epoch 183: Validation loss decreased from 0.057443 to 0.056763.\n",
      "Epoch 186: Validation loss decreased from 0.056763 to 0.046464.\n",
      "Epoch: 190 \tTraining Loss: 0.456276 \tValidation Loss: 0.062475\n",
      "Epoch: 200 \tTraining Loss: 0.468159 \tValidation Loss: 0.069350\n",
      "Epoch: 210 \tTraining Loss: 0.465059 \tValidation Loss: 0.066533\n",
      "Epoch   217: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 220 \tTraining Loss: 0.456766 \tValidation Loss: 0.059206\n",
      "Epoch: 230 \tTraining Loss: 0.465272 \tValidation Loss: 0.057584\n",
      "Epoch: 240 \tTraining Loss: 0.440277 \tValidation Loss: 0.055154\n",
      "Epoch   248: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 250 \tTraining Loss: 0.452743 \tValidation Loss: 0.056909\n",
      "Epoch: 260 \tTraining Loss: 0.445800 \tValidation Loss: 0.057793\n",
      "Epoch: 270 \tTraining Loss: 0.459878 \tValidation Loss: 0.058363\n",
      "Epoch   279: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 280 \tTraining Loss: 0.460202 \tValidation Loss: 0.057763\n",
      "Epoch: 290 \tTraining Loss: 0.448873 \tValidation Loss: 0.057706\n",
      "Epoch: 300 \tTraining Loss: 0.450912 \tValidation Loss: 0.057607\n",
      "\n",
      "FINISHED\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"jO3eWcSOMU9TI25k2qNW9O"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "output_df.head()"
   ],
   "execution_count":37,
   "outputs":[
    {
     "data":{
      "text\/html":[
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "<\/style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th><\/th>\n",
       "      <th>S<\/th>\n",
       "      <th>T<\/th>\n",
       "      <th>V_atm<\/th>\n",
       "      <th>Beta<\/th>\n",
       "      <th>Rho<\/th>\n",
       "      <th>Volvol<\/th>\n",
       "      <th>K<\/th>\n",
       "      <th>Lognormal_vol<\/th>\n",
       "      <th>is_train<\/th>\n",
       "      <th>pred_mapeloss<\/th>\n",
       "      <th>pred_maxloss<\/th>\n",
       "      <th>pred_mixedloss<\/th>\n",
       "    <\/tr>\n",
       "  <\/thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0<\/th>\n",
       "      <td>0.005<\/td>\n",
       "      <td>0.5<\/td>\n",
       "      <td>0.001<\/td>\n",
       "      <td>0.1<\/td>\n",
       "      <td>-0.4<\/td>\n",
       "      <td>0.0001<\/td>\n",
       "      <td>0.005000<\/td>\n",
       "      <td>0.198020<\/td>\n",
       "      <td>False<\/td>\n",
       "      <td>0.194161<\/td>\n",
       "      <td>0.221550<\/td>\n",
       "      <td>0.205326<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>1<\/th>\n",
       "      <td>0.005<\/td>\n",
       "      <td>0.5<\/td>\n",
       "      <td>0.001<\/td>\n",
       "      <td>0.1<\/td>\n",
       "      <td>-0.4<\/td>\n",
       "      <td>0.0001<\/td>\n",
       "      <td>0.013636<\/td>\n",
       "      <td>0.195679<\/td>\n",
       "      <td>True<\/td>\n",
       "      <td>0.192433<\/td>\n",
       "      <td>0.214536<\/td>\n",
       "      <td>0.203053<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>2<\/th>\n",
       "      <td>0.005<\/td>\n",
       "      <td>0.5<\/td>\n",
       "      <td>0.001<\/td>\n",
       "      <td>0.1<\/td>\n",
       "      <td>-0.4<\/td>\n",
       "      <td>0.0001<\/td>\n",
       "      <td>0.022273<\/td>\n",
       "      <td>0.193395<\/td>\n",
       "      <td>False<\/td>\n",
       "      <td>0.190705<\/td>\n",
       "      <td>0.208635<\/td>\n",
       "      <td>0.200780<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>3<\/th>\n",
       "      <td>0.005<\/td>\n",
       "      <td>0.5<\/td>\n",
       "      <td>0.001<\/td>\n",
       "      <td>0.1<\/td>\n",
       "      <td>-0.4<\/td>\n",
       "      <td>0.0001<\/td>\n",
       "      <td>0.030909<\/td>\n",
       "      <td>0.191166<\/td>\n",
       "      <td>False<\/td>\n",
       "      <td>0.188978<\/td>\n",
       "      <td>0.203251<\/td>\n",
       "      <td>0.198506<\/td>\n",
       "    <\/tr>\n",
       "    <tr>\n",
       "      <th>4<\/th>\n",
       "      <td>0.005<\/td>\n",
       "      <td>0.5<\/td>\n",
       "      <td>0.001<\/td>\n",
       "      <td>0.1<\/td>\n",
       "      <td>-0.4<\/td>\n",
       "      <td>0.0001<\/td>\n",
       "      <td>0.039545<\/td>\n",
       "      <td>0.188990<\/td>\n",
       "      <td>True<\/td>\n",
       "      <td>0.187250<\/td>\n",
       "      <td>0.200768<\/td>\n",
       "      <td>0.196233<\/td>\n",
       "    <\/tr>\n",
       "  <\/tbody>\n",
       "<\/table>\n",
       "<\/div>"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"BRRMJxSfercoT7PbeH6J4J"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "output_df.to_pickle(data_path + \"nn_pred_data_30patience.pkl\")"
   ],
   "execution_count":39,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"H9mngekfDmMewEu7xfSHWI"
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":0,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "type":"CODE",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "node_id":"73k7MsFUkEGUnMhSFTI4Yk"
    }
   }
  }
 ],
 "metadata":{
  "datalore":{
   "version":1,
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"default",
   "packages":[
    
   ]
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}